{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHA0lEQVR4nO3ZvU5VWQCG4X3wxMaAPYHGjtpWEqOJhVdjZ21p4YV4BV6AtZWFFwChNBHiTwxhT+VbDZnDDmfW6DxPe1bIRwL7ZbFX8zzPEwBM07QzegAA/x2iAEBEAYCIAgARBQAiCgBEFADIepNDV1dX09nZ2bS7uzutVqttbwLgls3zPF1cXEz7+/vTzs7194GNonB2djYdHh7e2jgAxjg5OZkODg6u/XyjKOzu7vbF9vb2bmcZAP+a8/Pz6fDwsOf5dTaKwq9/Ge3t7YkCwG/sn14BeNEMQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAWY8esG1fvnwZPWGRZ8+ejZ6w2MePH0dPWOT79++jJyzy5MmT0RMWefTo0egJi718+XL0hBv79u3bRufcFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCsRw/Ytvv374+esMjr169HT1js+fPnoycsMs/z6AmLfPr0afSERY6OjkZP+F+5vLzc6JybAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACDr0QP4e8fHx6MnLPbw4cPRExb5/Pnz6AmLHB0djZ7AH8RNAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMh69AD+3p07d0ZPWOz9+/ejJyzy9OnT0RMWWa9/z1/jd+/ejZ6w2M7On/v39J/7nQFwY6IAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAsh49AP4rXr16NXrCIsfHx6MnLHJ5eTl6wmJ3794dPWFr3BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGArG9yeJ7naZ7nbW3ZitVqNXrCIqenp6MnLPbgwYPRExb53X62f3n79u3oCYus1zd6/PAvcVMAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAZH2Twx8+fJju3bu3rS1b8fjx49ETFvn58+foCYu9efNm9IRFXrx4MXoCDOemAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFALLe5NA8z9M0TdPXr1+3OmYbfm3/3fyuu6dpmn78+DF6wiLn5+ejJ8DW/Pr5/qdny2re4Olzeno6HR4e3s4yAIY5OTmZDg4Orv18oyhcXV1NZ2dn0+7u7rRarW51IADbN8/zdHFxMe3v7087O9e/OdgoCgD8P3jRDEBEAYCIAgARBQAiCgBEFACIKACQvwDMppXmb6HR+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=5, flat=True)\n",
    "data_zoom = mnist.MNISTData(size_initial=20, size_final=8, color_depth=8, flat=True)\n",
    "\n",
    "x_train= data_zoom.x_train\n",
    "y_train= data_zoom.y_train\n",
    "x_test= data_zoom.x_test\n",
    "x_test= data_zoom.y_test\n",
    "\n",
    "ax = plt.subplot(1, 1 , 1)\n",
    "\n",
    "plt.imshow(x_train[0].reshape(size_final,size_final), cmap='gray_r')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0446\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0111\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0090\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0079\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0073\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0066\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0063\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0059\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0055\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e84f86ee20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "BIT_WIDTH = 8\n",
    "\n",
    "# # Replace these with your actual data\n",
    "# x_train = np.random.random((100, 10))\n",
    "# y_train = np.random.random((100, 10))\n",
    "\n",
    "input_shape = (x_train.shape[-1],)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=input_shape)\n",
    "encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "encoder_l2 = Dense(128, activation='relu')(encoder_l1)\n",
    "encoder_l3 = Dense(256, activation='relu')(encoder_l2)\n",
    "encoder_output = Dense(128, activation='relu')(encoder_l3)\n",
    "\n",
    "# Decoder\n",
    "decoder_l1 = Dense(256, activation='relu')(encoder_output)\n",
    "decoder_l2 = Dense(128, activation='relu')(decoder_l1)\n",
    "decoder_l3 = Dense(64, activation='relu')(decoder_l2)\n",
    "decoder_output = Dense(y_train.shape[-1], activation='sigmoid')(decoder_l3)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmp1tb4jal9\\assets\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "    for data in x_train:\n",
    "        yield [np.array([data], dtype=np.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "with open('quantized_model.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "model.save_weights('quantized_weights.h5')\n",
    "\n",
    "\n",
    "# Load the quantized model\n",
    "interpreter = tf.lite.Interpreter(model_path='quantized_model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Load the saved quantized weights\n",
    "model.load_weights('quantized_weights.h5')\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-123 -126 -117 -117 -123 -105 -117 -126   47 -105]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare input data\n",
    "input_data = np.array([x_train[0]], dtype=np.int8)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "BIT_WIDTH = 8\n",
    "\n",
    "# # Replace these with your actual data\n",
    "# x_train = np.random.random((100, 10))\n",
    "# y_train = np.random.random((100, 10))\n",
    "\n",
    "input_shape = (x_train.shape[-1],)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=input_shape)\n",
    "encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
    "encoder_l2 = Dense(128, activation='relu')(encoder_l1)\n",
    "encoder_l3 = Dense(256, activation='relu')(encoder_l2)\n",
    "encoder_output = Dense(128, activation='relu')(encoder_l3)\n",
    "\n",
    "# Decoder\n",
    "decoder_l1 = Dense(256, activation='relu')(encoder_output)\n",
    "decoder_l2 = Dense(128, activation='relu')(decoder_l1)\n",
    "decoder_l3 = Dense(64, activation='relu')(decoder_l2)\n",
    "decoder_output = Dense(y_train.shape[-1], activation='sigmoid')(decoder_l3)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Load the saved quantized weights\n",
    "model.load_weights('quantized_weights.h5')\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(x_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 10 into shape (8,8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# Display reconstructed MNIST\u001b[39;00m\n\u001b[0;32m     15\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, n, i \u001b[39m+\u001b[39m n \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m plt\u001b[39m.\u001b[39mimshow(reco_imgs[i]\u001b[39m.\u001b[39;49mreshape(\n\u001b[0;32m     17\u001b[0m     img_size, img_size), cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray_r\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m ax\u001b[39m.\u001b[39mget_xaxis()\u001b[39m.\u001b[39mset_visible(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m ax\u001b[39m.\u001b[39mget_yaxis()\u001b[39m.\u001b[39mset_visible(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 10 into shape (8,8)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 6\n",
    "plt.figure(figsize=(10, 3))\n",
    "reco_imgs = model.predict(test_imgs)\n",
    "img_size = int(np.sqrt(input_shape[0]))\n",
    "\n",
    "for i in range(n):\n",
    "    # Display original MNIST\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_imgs[i].reshape(\n",
    "        img_size, img_size), cmap='gray_r')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstructed MNIST\n",
    "    ax = plt.subplot(2, n, i + n + 1)\n",
    "    plt.imshow(reco_imgs[i].reshape(\n",
    "        img_size, img_size), cmap='gray_r')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.savefig(\n",
    "    './images/QAE/reconstructed images{model_name}.png'.format(model_name=\" complete\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b75f63a51ab1e44c10e89cf3b718812d9c5e2447d39cb402b946ba7653bfcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

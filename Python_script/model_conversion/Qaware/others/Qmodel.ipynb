{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=5, flat=True)\n",
    "data_zoom = mnist.MNISTData(size_initial=20, size_final=8, color_depth=8, flat=True)\n",
    "\n",
    "x_train= data_zoom.x_train\n",
    "y_train= data_zoom.y_train\n",
    "x_test= data_zoom.x_test\n",
    "x_test= data_zoom.y_test\n",
    "\n",
    "ax = plt.subplot(1, 1 , 1)\n",
    "\n",
    "plt.imshow(x_train[0].reshape(size_final,size_final), cmap='gray_r')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import qkeras as qk\n",
    "from tensorflow.python.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# Define the input shape\n",
    "# input_shape = (28, 28, 1)\n",
    "input_shape_var = (x_train.shape[-1],)\n",
    "\n",
    "# Create a sequential model\n",
    "# model = tf.keras.Sequential()\n",
    "# model = tf.keras.Sequential(input = input)\n",
    "# model = tf.keras.Model(input = input)\n",
    "\n",
    "input_layer  = Input(shape=input_shape_var, name='encoder_input')\n",
    "\n",
    "# Add a QDense layer with 4 output units\n",
    "# qk.qlayers.QDense(4, input_shape=input_shape))\n",
    "encoder = qk.qlayers.QDense(4, input_shape=input_shape_var)(input_layer)\n",
    "# Add a QActivation layer with relu activation\n",
    "encoder = qk.qlayers.QActivation('relu', name='relu1')(encoder)\n",
    "\n",
    "# Add a QDense layer with 3 output units\n",
    "encoder = qk.qlayers.QDense(3)(encoder)\n",
    "# Add a QActivation layer with relu activation\n",
    "encoder = qk.qlayers.QActivation('relu', name='relu2')(encoder)\n",
    "\n",
    "\n",
    "# Add a QDense layer with 2 output units\n",
    "encoder = qk.qlayers.QDense(2)(encoder)\n",
    "# # Add a QActivation layer with relu activation\n",
    "# encoder = qk.qlayers.QActivation('relu')\n",
    "\n",
    "# encoder = Dense(2,  activation='relu',\n",
    "#                         name='encoder_output')(encoder)\n",
    "# encoder_model = tf.keras.Model(encoder_input, encoder, name='encoder')\n",
    "\n",
    "# Add a Dense layer with 2 output units\n",
    "encoder_output = Dense(2, activation='relu', name='encoder_output')(encoder)\n",
    "encoder_model = tf.keras.Model(input_layer, encoder_output)\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "# # Create a separate input for the decoder\n",
    "# decoder_input = Input(shape=(2,), name='decoder_input')\n",
    "\n",
    "\n",
    "# Add a QDense layer with 3 output units\n",
    "decoder = qk.qlayers.QDense(3)(encoder_output)\n",
    "# Add a QActivation layer with relu activation\n",
    "decoder = qk.qlayers.QActivation('relu', name='relu3')(decoder)\n",
    "\n",
    "# Add a QDense layer with 4 output units\n",
    "decoder = qk.qlayers.QDense(4)(decoder)\n",
    "# Add a QActivation layer with relu activation\n",
    "decoder = qk.qlayers.QActivation('relu', name='relu4')(decoder)\n",
    "\n",
    "# # Add a QDense layer with 4 output units\n",
    "# decoder = qk.qlayers.QDense(input_shape[0])(decoder)\n",
    "# # Add a QActivation layer with softmax activation\n",
    "# decoder = qk.qlayers.QActivation('softmax', name='softmax')(decoder)\n",
    "\n",
    "# # Add a QDense layer with 4 output units\n",
    "# decoder_output = qk.qlayers.QDense(input_shape[0])(decoder)\n",
    "# # Add a QActivation layer with softmax activation\n",
    "# decoder_output = qk.qlayers.QActivation('softmax', name='softmax')(decoder_output)\n",
    "\n",
    "# # Connect the encoder and decoder\n",
    "# autoencoder = tf.keras.Model(inputs=[input_layer , decoder_input], outputs=decoder_output)\n",
    "# # Compile the model\n",
    "# autoencoder.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# # Print the model summary\n",
    "# autoencoder.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "autoencoder = tf.keras.Model(\n",
    "    input_layer, outputs=decoder, name='autoencoder')\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 12s 5ms/step - loss: 0.0181\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0089\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0071\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0064\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0057\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0052\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0048\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0045\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0042\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24931edb970>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Load your dataset here, e.g., x_train, y_train\n",
    "# ...\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (x_train.shape[-1],)\n",
    "\n",
    "# Quantize the layers\n",
    "def apply_quantization(layer):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=input_shape, name='encoder_input')\n",
    "encoder_layer1 = apply_quantization(Dense(128, activation='relu', name='encoder_layer1'))(encoder_input)\n",
    "encoder_layer2 = apply_quantization(Dense(64, activation='relu', name='encoder_layer2'))(encoder_layer1)\n",
    "encoder_layer3 = apply_quantization(Dense(32, activation='relu', name='encoder_layer3'))(encoder_layer2)\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(32,), name='decoder_input')\n",
    "decoder_concat = Concatenate()([encoder_layer3, decoder_input])\n",
    "decoder_layer1 = apply_quantization(Dense(64, activation='relu', name='decoder_layer1'))(decoder_concat)\n",
    "decoder_layer2 = apply_quantization(Dense(128, activation='relu', name='decoder_layer2'))(decoder_layer1)\n",
    "decoder_output = Dense(y_train.shape[-1], activation='linear', name='decoder_output')(decoder_layer2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
    "\n",
    "# Apply quantization\n",
    "model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "# Assume the decoder's input is the same as the encoder's output during training\n",
    "model.fit([x_train, np.zeros((x_train.shape[0], 32))], y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 10s 4ms/step - loss: 0.0182\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0087\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0071\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0062\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0055\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0050\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0046\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0043\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0040\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249321bbd60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Load your dataset here, e.g., x_train, y_train\n",
    "# ...\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (x_train.shape[-1],)\n",
    "\n",
    "# Quantize the layers\n",
    "def apply_quantization(layer):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=input_shape, name='encoder_input')\n",
    "encoder_layer1 = apply_quantization(Dense(128, activation='relu', name='encoder_layer1'))(encoder_input)\n",
    "encoder_layer2 = apply_quantization(Dense(64, activation='relu', name='encoder_layer2'))(encoder_layer1)\n",
    "encoder_output = apply_quantization(Dense(32, activation='relu', name='encoder_layer3'))(encoder_layer2)\n",
    "\n",
    "# Decoder\n",
    "decoder_layer4 = apply_quantization(Dense(64, activation='relu', name='decoder_layer4'))(encoder_output)\n",
    "decoder_layer5 = apply_quantization(Dense(128, activation='relu', name='decoder_layer5'))(decoder_layer4)\n",
    "decoder_output = Dense(y_train.shape[-1], activation='linear', name='decoder_output')(decoder_layer5)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "\n",
    "# Apply quantization\n",
    "model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer encoder_input:\n",
      "  No weights and biases.\n",
      "Layer quantize_layer_2:\n",
      "  No weights and biases.\n",
      "Layer quant_encoder_layer1:\n",
      "  No weights and biases.\n",
      "Layer quant_encoder_layer2:\n",
      "  No weights and biases.\n",
      "Layer quant_encoder_layer3:\n",
      "  No weights and biases.\n",
      "Layer quant_decoder_layer4:\n",
      "  No weights and biases.\n",
      "Layer quant_decoder_layer5:\n",
      "  No weights and biases.\n",
      "Layer decoder_output:\n",
      "  Weights:\n",
      "[[-0.20701064 -0.17265531  0.07631456 ...  0.0121364   0.17497708\n",
      "   0.05646818]\n",
      " [ 0.21986058 -0.08627374 -0.05961151 ... -0.02268347  0.06751335\n",
      "  -0.07447217]\n",
      " [-0.09029378  0.14219518  0.12566659 ... -0.12262254 -0.12713194\n",
      "  -0.13434418]\n",
      " ...\n",
      " [ 0.11256388 -0.17482634 -0.05767298 ...  0.17343871  0.0959226\n",
      "   0.24154058]\n",
      " [-0.05519816 -0.08447433 -0.1041726  ...  0.02123571  0.01905223\n",
      "  -0.07375693]\n",
      " [-0.02484782  0.10674526  0.08047733 ...  0.08869215  0.10797627\n",
      "   0.09085944]]\n",
      "  Biases:\n",
      "[0.06200311 0.09120873 0.06115634 0.08244468 0.04640969 0.07651157\n",
      " 0.03705138 0.03694129 0.0598756  0.10222141]\n"
     ]
    }
   ],
   "source": [
    "# Get the weights and biases of the loaded model\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(f'Layer {layer.name}:')\n",
    "    \n",
    "    if len(weights) == 2:\n",
    "        w, b = weights\n",
    "        print(f'  Weights:\\n{w}')\n",
    "        print(f'  Biases:\\n{b}')\n",
    "    else:\n",
    "        print('  No weights and biases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: 'QuantizeLayer'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load the model from the saved file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loaded_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mmy_model.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\luisa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\luisa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\saving\\legacy\\serialization.py:385\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m object_registration\u001b[39m.\u001b[39mget_registered_object(\n\u001b[0;32m    382\u001b[0m     class_name, custom_objects, module_objects\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 385\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    386\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown \u001b[39m\u001b[39m{\u001b[39;00mprintable_module_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    387\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand that this object is included in the scope. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    390\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#registering_the_custom_object for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m     )\n\u001b[0;32m    393\u001b[0m cls_config \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    394\u001b[0m \u001b[39m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[39m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[39m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[39m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[39m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown layer: 'QuantizeLayer'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "# Load the model from the saved file\n",
    "loaded_model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights and biases of the loaded model\n",
    "for layer in loaded_model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(f'Layer {layer.name}:')\n",
    "    \n",
    "    if len(weights) == 2:\n",
    "        w, b = weights\n",
    "        print(f'  Weights:\\n{w}')\n",
    "        print(f'  Biases:\\n{b}')\n",
    "    else:\n",
    "        print('  No weights and biases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import qkeras as qk\n",
    "# from tensorflow.python.keras import Input\n",
    "\n",
    "# # Define the input shape\n",
    "# # input_shape = (28, 28, 1)\n",
    "# input_shape = (x_train.shape[-1],)\n",
    "\n",
    "# # Create a sequential model\n",
    "# model = tf.keras.Sequential()\n",
    "# # model = tf.keras.Sequential(input = input)\n",
    "# # model = tf.keras.Model(input = input)\n",
    "\n",
    "# # input = Input(shape=input_shape, name='encoder_input')\n",
    "# # model.add(input)\n",
    "\n",
    "# # Add a QDense layer with 4 output units\n",
    "# # model.add(qk.qlayers.QDense(4, input_shape=input_shape))\n",
    "# model.add(qk.qlayers.QDense(4, input_shape=input_shape))\n",
    "# # Add a QActivation layer with relu activation\n",
    "# model.add(qk.qlayers.QActivation('relu'))\n",
    "\n",
    "# # Add a QDense layer with 3 output units\n",
    "# model.add(qk.qlayers.QDense(3))\n",
    "# # Add a QActivation layer with relu activation\n",
    "# model.add(qk.qlayers.QActivation('relu'))\n",
    "\n",
    "\n",
    "# # Add a QDense layer with 2 output units\n",
    "# model.add(qk.qlayers.QDense(2))\n",
    "# # Add a QActivation layer with relu activation\n",
    "# model.add(qk.qlayers.QActivation('relu'))\n",
    "\n",
    "# # Add a QDense layer with 3 output units\n",
    "# model.add(qk.qlayers.QDense(3))\n",
    "# # Add a QActivation layer with relu activation\n",
    "# model.add(qk.qlayers.QActivation('relu'))\n",
    "\n",
    "# # Add a QDense layer with 4 output units\n",
    "# model.add(qk.qlayers.QDense(4))\n",
    "# # Add a QActivation layer with relu activation\n",
    "# model.add(qk.qlayers.QActivation('relu'))\n",
    "\n",
    "# # Add a QDense layer with 4 output units\n",
    "# model.add(qk.qlayers.QDense(input_shape[0]))\n",
    "# # Add a QActivation layer with softmax activation\n",
    "# model.add(qk.qlayers.QActivation('softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# model.fit(x_train, Y_train,\n",
    "#           batch_size=128,\n",
    "#           epochs=10,\n",
    "#           validation_data=(X_test, Y_test))\n",
    "\n",
    "# # Save the model\n",
    "# model.save('my_model.h5')\n",
    "\n",
    "# # Load the model\n",
    "# loaded_model = tf.keras.models.load_model('my_model.h5', custom_objects={'QActivation': qk.qlayers.QActivation})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b75f63a51ab1e44c10e89cf3b718812d9c5e2447d39cb402b946ba7653bfcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=5, flat=True)\n",
    "data_zoom = mnist.MNISTData(size_initial=20, size_final=8, color_depth=8, flat=True)\n",
    "\n",
    "x_train= data_zoom.x_train\n",
    "y_train= data_zoom.y_train\n",
    "x_test= data_zoom.x_test\n",
    "x_test= data_zoom.y_test\n",
    "\n",
    "ax = plt.subplot(1, 1 , 1)\n",
    "\n",
    "plt.imshow(x_train[0].reshape(size_final,size_final), cmap='gray_r')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Load your dataset here, e.g., x_train, y_train\n",
    "# ...\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (x_train.shape[-1],)\n",
    "\n",
    "# Quantize the layers\n",
    "def apply_quantization(layer):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=input_shape, name='encoder_input')\n",
    "encoder_layer1 = apply_quantization(Dense(128, activation='relu', name='encoder_layer1'))(encoder_input)\n",
    "encoder_layer2 = apply_quantization(Dense(64, activation='relu', name='encoder_layer2'))(encoder_layer1)\n",
    "encoder_layer3 = apply_quantization(Dense(32, activation='relu', name='encoder_layer3'))(encoder_layer2)\n",
    "\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(32,), name='decoder_input')\n",
    "decoder_concat = Concatenate()([encoder_layer3, decoder_input])\n",
    "decoder_layer1 = apply_quantization(Dense(64, activation='relu', name='decoder_layer1'))(decoder_concat)\n",
    "decoder_layer2 = apply_quantization(Dense(128, activation='relu', name='decoder_layer2'))(decoder_layer1)\n",
    "decoder_output = Dense(y_train.shape[-1], activation='linear', name='decoder_output')(decoder_layer2)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
    "\n",
    "# Apply quantization\n",
    "model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "# Assume the decoder's input is the same as the encoder's output during training\n",
    "model.fit([x_train, np.zeros((x_train.shape[0], 32))], y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Load your dataset here, e.g., x_train, y_train\n",
    "# ...\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (x_train.shape[-1],)\n",
    "\n",
    "# Quantize the layers\n",
    "def apply_quantization(layer):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=input_shape, name='encoder_input')\n",
    "encoder_layer1 = apply_quantization(Dense(128, activation='relu', name='encoder_layer1'))(encoder_input)\n",
    "encoder_layer2 = apply_quantization(Dense(64, activation='relu', name='encoder_layer2'))(encoder_layer1)\n",
    "encoder_output = apply_quantization(Dense(32, activation='relu', name='encoder_layer3'))(encoder_layer2)\n",
    "\n",
    "# Decoder\n",
    "decoder_layer4 = apply_quantization(Dense(64, activation='relu', name='decoder_layer4'))(encoder_output)\n",
    "decoder_layer5 = apply_quantization(Dense(128, activation='relu', name='decoder_layer5'))(decoder_layer4)\n",
    "decoder_output = Dense(y_train.shape[-1], activation='linear', name='decoder_output')(decoder_layer5)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "\n",
    "# Apply quantization\n",
    "model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights and biases of the loaded model\n",
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(f'Layer {layer.name}:')\n",
    "    \n",
    "    if len(weights) == 2:\n",
    "        w, b = weights\n",
    "        print(f'  Weights:\\n{w}')\n",
    "        print(f'  Biases:\\n{b}')\n",
    "    else:\n",
    "        print('  No weights and biases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights and biases of the loaded model\n",
    "for layer in loaded_model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(f'Layer {layer.name}:')\n",
    "    \n",
    "    if len(weights) == 2:\n",
    "        w, b = weights\n",
    "        print(f'  Weights:\\n{w}')\n",
    "        print(f'  Biases:\\n{b}')\n",
    "    else:\n",
    "        print('  No weights and biases.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import MNIST_database as mnist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "#Choose the final size of your image dataset\n",
    "size_final = 8\n",
    "\n",
    "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=5, flat=True)\n",
    "data_zoom = mnist.MNISTData(size_initial=20, size_final=8, color_depth=8, flat=True)\n",
    "\n",
    "x_train= data_zoom.x_train\n",
    "y_train= data_zoom.y_train\n",
    "x_test= data_zoom.x_test\n",
    "x_test= data_zoom.y_test\n",
    "\n",
    "ax = plt.subplot(1, 1 , 1)\n",
    "\n",
    "plt.imshow(x_train[0].reshape(size_final,size_final), cmap='gray_r')\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 12s 5ms/step - loss: 0.1000\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1000\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1000\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1000\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1000\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1000\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1000\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1000\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1000\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21034e8ba90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset here, e.g., x_train, y_train\n",
    "# ...\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (x_train.shape[-1],)\n",
    "\n",
    "# Set bit width\n",
    "BIT_WIDTH = 5\n",
    "\n",
    "# Define custom quantization function\n",
    "def custom_quantization(x, min_val, max_val):\n",
    "    return tf.quantization.fake_quant_with_min_max_vars(x, min_val, max_val, num_bits=BIT_WIDTH)\n",
    "\n",
    "# Create custom layer with custom quantization\n",
    "class CustomQuantizedDense(Layer):\n",
    "    def __init__(self, units, activation, **kwargs):\n",
    "        super(CustomQuantizedDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", (input_shape[-1], self.units))\n",
    "        self.bias = self.add_weight(\"bias\", (self.units,))\n",
    "        self.min_val = self.add_weight(\"min_val\", shape=(), trainable=False)\n",
    "        self.max_val = self.add_weight(\"max_val\", shape=(), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        quantized_kernel = custom_quantization(self.kernel, self.min_val, self.max_val)\n",
    "        quantized_bias = custom_quantization(self.bias, self.min_val, self.max_val)\n",
    "        output = tf.matmul(inputs, quantized_kernel) + quantized_bias\n",
    "        return self.activation(output)\n",
    "\n",
    "# Encoder\n",
    "encoder_input = Input(shape=input_shape, name='encoder_input')\n",
    "encoder_layer1 = CustomQuantizedDense(128, activation=tf.nn.relu, name='encoder_layer1')(encoder_input)\n",
    "encoder_layer2 = CustomQuantizedDense(64, activation=tf.nn.relu, name='encoder_layer2')(encoder_layer1)\n",
    "encoder_output = CustomQuantizedDense(32, activation=tf.nn.relu, name='encoder_layer3')(encoder_layer2)\n",
    "\n",
    "# Decoder\n",
    "decoder_layer4 = CustomQuantizedDense(64, activation=tf.nn.relu, name='decoder_layer4')(encoder_output)\n",
    "decoder_layer5 = CustomQuantizedDense(128, activation=tf.nn.relu, name='decoder_layer5')(decoder_layer4)\n",
    "decoder_output = CustomQuantizedDense(y_train.shape[-1], activation=tf.nn.relu, name='decoder_output')(decoder_layer5)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your QAT model is stored in the variable `qat_model`\n",
    "qat_model = model\n",
    "# Convert the QAT model to a fully quantized model\n",
    "quantized_model = torch.quantization.convert(qat_model)\n",
    "\n",
    "# Iterate through the layers and obtain quantized weights\n",
    "for name, module in quantized_model.named_modules():\n",
    "    if hasattr(module, 'weight'):\n",
    "        quantized_weight = module.weight()\n",
    "        print(f\"Layer: {name}, Quantized Weight: {quantized_weight}, Dtype: {quantized_weight.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer encoder_input:\n",
      "  No weights and biases.\n",
      "Layer encoder_layer1:\n",
      "  Weights:\n",
      "<tf.Variable 'encoder_layer1/kernel:0' shape=(64, 128) dtype=float32, numpy=\n",
      "array([[ 0.16362126, -0.16021436, -0.13087237, ...,  0.01030259,\n",
      "         0.10924523, -0.06339029],\n",
      "       [ 0.02260731, -0.10997149, -0.15902293, ...,  0.15956075,\n",
      "         0.14734708, -0.0841625 ],\n",
      "       [ 0.05756378, -0.17191501, -0.10418157, ...,  0.11823379,\n",
      "         0.03678961,  0.15261434],\n",
      "       ...,\n",
      "       [-0.1450727 , -0.13419013, -0.09763443, ...,  0.08610259,\n",
      "        -0.07819551,  0.13991414],\n",
      "       [-0.11880619, -0.09985001, -0.08469439, ...,  0.09752624,\n",
      "        -0.1198956 ,  0.11012895],\n",
      "       [-0.14261287,  0.01854697,  0.0420379 , ..., -0.00986001,\n",
      "         0.0053416 ,  0.15654297]], dtype=float32)>\n",
      "  Biases:\n",
      "<tf.Variable 'encoder_layer1/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([ 0.02863955,  0.0500284 ,  0.00888328,  0.13266937,  0.05329411,\n",
      "        0.04863629,  0.09857111,  0.00917105, -0.0209738 , -0.14651391,\n",
      "       -0.1106218 ,  0.03820483,  0.10828196,  0.01391502, -0.121178  ,\n",
      "       -0.09807393,  0.12824912, -0.11026293, -0.14699794, -0.09382041,\n",
      "        0.07023051, -0.0699226 ,  0.10331453,  0.07530643, -0.03738074,\n",
      "        0.00276306, -0.00133298,  0.1341898 , -0.00492355,  0.11035772,\n",
      "       -0.09280428, -0.05642054,  0.12798117,  0.14270829,  0.0438422 ,\n",
      "        0.12065016,  0.03927091,  0.13205425, -0.1266278 ,  0.01139508,\n",
      "       -0.07994021,  0.07126224, -0.08233747, -0.07522156, -0.03135126,\n",
      "       -0.09868866, -0.03285033, -0.05419783,  0.11153205,  0.0377714 ,\n",
      "       -0.06513038, -0.13893417, -0.08672056,  0.13523297, -0.01032268,\n",
      "       -0.04658888,  0.00248654,  0.07961904,  0.05598068,  0.04569107,\n",
      "        0.1514997 , -0.14824884, -0.13420983,  0.14002438, -0.00228992,\n",
      "       -0.0086676 ,  0.08670534, -0.01050819,  0.09403017,  0.10202898,\n",
      "       -0.11516805, -0.13763244, -0.0757587 , -0.08833431, -0.06015339,\n",
      "        0.14987262,  0.07780209,  0.15264244,  0.05184494, -0.09174529,\n",
      "       -0.13218129, -0.15201454, -0.09191728,  0.09886555, -0.0277423 ,\n",
      "        0.06401251,  0.13618712, -0.06416152,  0.01871178,  0.14149846,\n",
      "       -0.13653281,  0.11990483, -0.03454401,  0.02332635,  0.00790647,\n",
      "       -0.0978258 ,  0.07807475, -0.08473345, -0.05908543, -0.12311254,\n",
      "       -0.13576458,  0.0741628 , -0.12413225, -0.0934753 ,  0.06410031,\n",
      "        0.13537051, -0.02436954,  0.13762347,  0.13057737, -0.02346039,\n",
      "        0.02656357,  0.0261441 ,  0.10327859,  0.1346379 , -0.02470709,\n",
      "        0.09505697, -0.02079138,  0.04885539, -0.09817193, -0.07679929,\n",
      "       -0.03617999, -0.00891303, -0.14362313,  0.06306042, -0.09725982,\n",
      "        0.13707547,  0.02850927,  0.10951363], dtype=float32)>\n",
      "Layer encoder_layer2:\n",
      "  Weights:\n",
      "<tf.Variable 'encoder_layer2/kernel:0' shape=(128, 64) dtype=float32, numpy=\n",
      "array([[-0.13621823, -0.10075233, -0.08302124, ...,  0.06251363,\n",
      "        -0.08172649, -0.16676196],\n",
      "       [-0.11403942, -0.0094119 ,  0.04730208, ...,  0.04985845,\n",
      "        -0.12087897,  0.02316113],\n",
      "       [-0.12577197,  0.17324074,  0.09148674, ...,  0.15492018,\n",
      "         0.03488287,  0.07446294],\n",
      "       ...,\n",
      "       [-0.1397428 ,  0.00231922, -0.15788652, ..., -0.07449506,\n",
      "        -0.09652239, -0.03240916],\n",
      "       [-0.05417282,  0.11143236,  0.1385984 , ...,  0.05559906,\n",
      "         0.10483758,  0.07680939],\n",
      "       [ 0.10352118,  0.12013336,  0.05587235, ..., -0.0780689 ,\n",
      "         0.13246344,  0.03728339]], dtype=float32)>\n",
      "  Biases:\n",
      "<tf.Variable 'encoder_layer2/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([-0.16642706, -0.14041881,  0.20930113, -0.17918158, -0.20564362,\n",
      "        0.04309361,  0.07284863,  0.10682787, -0.20647866, -0.0078284 ,\n",
      "        0.03380327,  0.13341911, -0.01197703,  0.08105619,  0.20874535,\n",
      "       -0.03458013, -0.19365719,  0.10665314,  0.06633259, -0.10165408,\n",
      "       -0.20457768,  0.09756656,  0.04064651, -0.09839423,  0.02678671,\n",
      "       -0.1789031 ,  0.00993255,  0.13018201,  0.1494648 ,  0.12349884,\n",
      "       -0.09479953,  0.17879288, -0.03340925, -0.04908392,  0.19952898,\n",
      "        0.04313548,  0.07564367,  0.21067302,  0.19903667, -0.14945742,\n",
      "       -0.01559135,  0.15435795,  0.0121848 ,  0.03397505,  0.2110642 ,\n",
      "        0.21116315,  0.1044661 ,  0.10860439, -0.20138405, -0.14708346,\n",
      "        0.14854847, -0.06075582, -0.01879458,  0.13997142, -0.11769535,\n",
      "       -0.17841463,  0.1939442 ,  0.14963757, -0.02636406, -0.03196459,\n",
      "       -0.19445589,  0.00657658,  0.14072688, -0.03827198], dtype=float32)>\n",
      "Layer encoder_layer3:\n",
      "  Weights:\n",
      "<tf.Variable 'encoder_layer3/kernel:0' shape=(64, 32) dtype=float32, numpy=\n",
      "array([[-0.17569107, -0.2052142 ,  0.14475405, ...,  0.10436457,\n",
      "         0.21361208, -0.00942677],\n",
      "       [ 0.09572595, -0.03978813,  0.11055189, ..., -0.13374716,\n",
      "        -0.23085302, -0.1501106 ],\n",
      "       [ 0.16475922, -0.15212804,  0.12431061, ..., -0.22861028,\n",
      "         0.05350214, -0.13286376],\n",
      "       ...,\n",
      "       [-0.07627571, -0.05823874,  0.15828496, ...,  0.18884182,\n",
      "        -0.07459009, -0.1374585 ],\n",
      "       [ 0.05433112, -0.01852709, -0.17267042, ..., -0.13639122,\n",
      "        -0.10316175, -0.08239233],\n",
      "       [-0.23359114, -0.03190655,  0.15059781, ...,  0.09272987,\n",
      "         0.24715602,  0.0008533 ]], dtype=float32)>\n",
      "  Biases:\n",
      "<tf.Variable 'encoder_layer3/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([ 0.14573014,  0.09515291,  0.26790103,  0.2552859 ,  0.24740419,\n",
      "        0.08370629, -0.028936  , -0.22797307, -0.2961832 , -0.0443297 ,\n",
      "       -0.30200616,  0.25632426,  0.10123071, -0.2905055 ,  0.0113318 ,\n",
      "        0.04965261, -0.09624866,  0.02995801, -0.24015918, -0.21526077,\n",
      "       -0.08291702,  0.27366766, -0.05467635, -0.20990327, -0.30610302,\n",
      "       -0.22403608,  0.00835747, -0.22785059,  0.23200676,  0.01577103,\n",
      "       -0.02708304, -0.21949013], dtype=float32)>\n",
      "Layer decoder_layer4:\n",
      "  Weights:\n",
      "<tf.Variable 'decoder_layer4/kernel:0' shape=(32, 64) dtype=float32, numpy=\n",
      "array([[ 0.22008556, -0.00911772, -0.1632908 , ..., -0.23119444,\n",
      "         0.2273733 ,  0.2398063 ],\n",
      "       [ 0.11464238, -0.2178756 ,  0.22099793, ..., -0.10863644,\n",
      "         0.18599701, -0.23901254],\n",
      "       [ 0.09574103, -0.08220756, -0.16112852, ...,  0.15789014,\n",
      "        -0.15585583, -0.06455725],\n",
      "       ...,\n",
      "       [ 0.03010607,  0.02125043,  0.15305817, ..., -0.18358278,\n",
      "         0.00494295, -0.08273113],\n",
      "       [ 0.03857458,  0.10245031, -0.07646602, ..., -0.09597898,\n",
      "         0.22985983, -0.19646072],\n",
      "       [-0.15847206, -0.0197885 ,  0.02457511, ..., -0.20682114,\n",
      "        -0.12539178,  0.1664061 ]], dtype=float32)>\n",
      "  Biases:\n",
      "<tf.Variable 'decoder_layer4/bias:0' shape=(64,) dtype=float32, numpy=\n",
      "array([-0.10209837,  0.02191934, -0.08725271,  0.09528722, -0.00717233,\n",
      "       -0.1773265 , -0.1678262 ,  0.11942439, -0.00124191, -0.15519531,\n",
      "       -0.17315975, -0.18679948,  0.07652567,  0.069575  , -0.07312083,\n",
      "        0.04289939, -0.06864876, -0.16115505, -0.16738707,  0.002077  ,\n",
      "       -0.00444627, -0.16866966,  0.05631085, -0.1104171 , -0.10440672,\n",
      "        0.08995812, -0.06689969, -0.14313409, -0.12292854,  0.08727311,\n",
      "        0.1606694 , -0.13718379,  0.12643535, -0.06887186, -0.19472627,\n",
      "       -0.19624531, -0.15576622, -0.081839  ,  0.17541252, -0.02193679,\n",
      "       -0.13733509, -0.12208901,  0.1426179 ,  0.06816085,  0.02574545,\n",
      "        0.10655512,  0.10632716, -0.05822746,  0.03584106, -0.14350888,\n",
      "       -0.14388411,  0.09693743,  0.13691841, -0.14461116,  0.03902738,\n",
      "        0.20322768, -0.16433772, -0.0999507 ,  0.06981064, -0.21377224,\n",
      "       -0.04976286, -0.20529746, -0.14409998,  0.00210343], dtype=float32)>\n",
      "Layer decoder_layer5:\n",
      "  Weights:\n",
      "<tf.Variable 'decoder_layer5/kernel:0' shape=(64, 128) dtype=float32, numpy=\n",
      "array([[ 0.06901778,  0.05882515,  0.11946081, ...,  0.06330958,\n",
      "         0.06887436,  0.14924379],\n",
      "       [-0.10399183,  0.0794474 ,  0.10278456, ..., -0.15539421,\n",
      "        -0.12788285,  0.03596716],\n",
      "       [ 0.10580076,  0.00386183, -0.00936908, ..., -0.00719957,\n",
      "         0.17337169, -0.10296993],\n",
      "       ...,\n",
      "       [ 0.1324002 , -0.09957775,  0.0490258 , ..., -0.15967566,\n",
      "         0.05163182,  0.04062071],\n",
      "       [-0.03736566,  0.0068057 ,  0.08795772, ..., -0.10051648,\n",
      "        -0.02024706, -0.09958175],\n",
      "       [-0.07509228, -0.11600393, -0.16134985, ..., -0.14790858,\n",
      "        -0.07210297, -0.06478594]], dtype=float32)>\n",
      "  Biases:\n",
      "<tf.Variable 'decoder_layer5/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([-0.04722814, -0.10753822,  0.10782306, -0.11087821,  0.01801148,\n",
      "        0.05295955,  0.10134752, -0.05628421, -0.05495243,  0.03498307,\n",
      "       -0.12982027,  0.09838401, -0.05975926,  0.02726233,  0.0143054 ,\n",
      "       -0.0787361 ,  0.13030188, -0.12707059,  0.02862929, -0.10722198,\n",
      "        0.11340924, -0.11336114,  0.08114508, -0.02389744,  0.1230426 ,\n",
      "        0.02770357,  0.04020751, -0.09046158, -0.07102856, -0.00309825,\n",
      "        0.03268279,  0.07247247,  0.08992846,  0.07352923,  0.12574188,\n",
      "       -0.00331403, -0.13849345, -0.13027027,  0.00190239,  0.12196751,\n",
      "        0.04433432, -0.04122129, -0.09534542, -0.14600499,  0.08212091,\n",
      "       -0.02235319,  0.02832726,  0.10575782, -0.05806433, -0.03455807,\n",
      "        0.02668835,  0.13354523,  0.14614259,  0.09636195,  0.09204449,\n",
      "        0.13786711, -0.09216578,  0.12075482, -0.03589744,  0.01948091,\n",
      "       -0.07147945, -0.1366668 ,  0.11070381, -0.0796263 , -0.02901667,\n",
      "       -0.05373478, -0.12977275, -0.09195349, -0.10768659,  0.00217195,\n",
      "        0.08069982,  0.14148055, -0.08649057, -0.06940598,  0.11376779,\n",
      "       -0.07615265, -0.07158307,  0.10400389,  0.10410975, -0.05863965,\n",
      "        0.01898228,  0.05126496, -0.10853818,  0.07448718, -0.07075547,\n",
      "        0.09187663, -0.01886989,  0.12971963,  0.05333361,  0.13514654,\n",
      "       -0.12611601, -0.13291918,  0.06577471,  0.08706082,  0.03024727,\n",
      "        0.01755771, -0.01284868,  0.04869877,  0.02805756,  0.1382647 ,\n",
      "        0.0815741 ,  0.13987212, -0.02614108,  0.08668844,  0.06926472,\n",
      "        0.07144557, -0.15024161, -0.11009875,  0.07085449,  0.09601842,\n",
      "       -0.13372171, -0.09807042,  0.14584418, -0.13971388, -0.08201455,\n",
      "       -0.0915859 , -0.05366277, -0.02149075,  0.07019158,  0.14889841,\n",
      "        0.06719391, -0.08239397, -0.12624484,  0.02932751, -0.10195722,\n",
      "       -0.05448928, -0.12202375,  0.11491339], dtype=float32)>\n",
      "Layer decoder_output:\n",
      "  Weights:\n",
      "<tf.Variable 'decoder_output/kernel:0' shape=(128, 10) dtype=float32, numpy=\n",
      "array([[ 0.07823576, -0.15464985, -0.0588665 , ...,  0.00939424,\n",
      "         0.02818479, -0.19702373],\n",
      "       [ 0.13899715,  0.12320818,  0.1796663 , ...,  0.16202305,\n",
      "         0.09603979, -0.18196848],\n",
      "       [ 0.19103761,  0.03461237,  0.10937537, ...,  0.0869173 ,\n",
      "         0.18716417, -0.05547404],\n",
      "       ...,\n",
      "       [ 0.07183771,  0.16592829, -0.19329292, ..., -0.03695539,\n",
      "         0.2040012 , -0.0058504 ],\n",
      "       [-0.01991889, -0.14592858, -0.1909096 , ...,  0.16288225,\n",
      "         0.13053568,  0.0297378 ],\n",
      "       [ 0.00719829,  0.10312299, -0.13909036, ...,  0.0907317 ,\n",
      "         0.00900568, -0.15709859]], dtype=float32)>\n",
      "  Biases:\n",
      "<tf.Variable 'decoder_output/bias:0' shape=(10,) dtype=float32, numpy=\n",
      "array([-0.23293799, -0.4954015 , -0.21756813, -0.30061066, -0.3866099 ,\n",
      "        0.08716309,  0.0166055 ,  0.17204934, -0.27576604,  0.30141443],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Get the weights and biases of the loaded model\n",
    "for layer in model.layers:\n",
    "    print(f'Layer {layer.name}:')\n",
    "    \n",
    "    if isinstance(layer, CustomQuantizedDense):\n",
    "        w, b = layer.kernel, layer.bias\n",
    "        print(f'  Weights:\\n{w}')\n",
    "        print(f'  Biases:\\n{b}')\n",
    "    else:\n",
    "        weights = layer.get_weights()\n",
    "        if len(weights) == 2:\n",
    "            w, b = weights\n",
    "            print(f'  Weights:\\n{w}')\n",
    "            print(f'  Biases:\\n{b}')\n",
    "        else:\n",
    "            print('  No weights and biases.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "\nLayer CustomQuantizedDense was created by passing\nnon-serializable argument values in `__init__()`,\nand therefore the layer must override `get_config()` in\norder to be serializable. Please implement `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2, **kwargs):\n        super().__init__(**kwargs)\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Save the model to a file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39mmy_model.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\luisa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\luisa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\base_layer.py:818\u001b[0m, in \u001b[0;36mLayer.get_config\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[39mreturn\u001b[39;00m config\n\u001b[0;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 818\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    819\u001b[0m         textwrap\u001b[39m.\u001b[39mdedent(\n\u001b[0;32m    820\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    821\u001b[0m \u001b[39mLayer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m was created by passing\u001b[39m\n\u001b[0;32m    822\u001b[0m \u001b[39mnon-serializable argument values in `__init__()`,\u001b[39m\n\u001b[0;32m    823\u001b[0m \u001b[39mand therefore the layer must override `get_config()` in\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39morder to be serializable. Please implement `get_config()`.\u001b[39m\n\u001b[0;32m    825\u001b[0m \n\u001b[0;32m    826\u001b[0m \u001b[39mExample:\u001b[39m\n\u001b[0;32m    827\u001b[0m \n\u001b[0;32m    828\u001b[0m \u001b[39mclass CustomLayer(keras.layers.Layer):\u001b[39m\n\u001b[0;32m    829\u001b[0m \u001b[39m    def __init__(self, arg1, arg2, **kwargs):\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[39m        super().__init__(**kwargs)\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[39m        self.arg1 = arg1\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[39m        self.arg2 = arg2\u001b[39m\n\u001b[0;32m    833\u001b[0m \n\u001b[0;32m    834\u001b[0m \u001b[39m    def get_config(self):\u001b[39m\n\u001b[0;32m    835\u001b[0m \u001b[39m        config = super().get_config()\u001b[39m\n\u001b[0;32m    836\u001b[0m \u001b[39m        config.update(\u001b[39m\u001b[39m{{\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[39m            \u001b[39m\u001b[39m\"\u001b[39m\u001b[39marg1\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: self.arg1,\u001b[39m\n\u001b[0;32m    838\u001b[0m \u001b[39m            \u001b[39m\u001b[39m\"\u001b[39m\u001b[39marg2\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: self.arg2,\u001b[39m\n\u001b[0;32m    839\u001b[0m \u001b[39m        \u001b[39m\u001b[39m}}\u001b[39;00m\u001b[39m)\u001b[39m\n\u001b[0;32m    840\u001b[0m \u001b[39m        return config\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         )\n\u001b[0;32m    842\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: \nLayer CustomQuantizedDense was created by passing\nnon-serializable argument values in `__init__()`,\nand therefore the layer must override `get_config()` in\norder to be serializable. Please implement `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2, **kwargs):\n        super().__init__(**kwargs)\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config"
     ]
    }
   ],
   "source": [
    "# Save the model to a file\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved file\n",
    "loaded_model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b75f63a51ab1e44c10e89cf3b718812d9c5e2447d39cb402b946ba7653bfcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import MNIST_database as mnist\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "\n",
                "#Choose the final size of your image dataset\n",
                "size_final = 8\n",
                "\n",
                "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=5, flat=True)\n",
                "data_zoom = mnist.MNISTData(size_initial=20, size_final=8, color_depth=8, flat=True)\n",
                "\n",
                "# x_train= data_zoom.x_train\n",
                "# y_train= data_zoom.y_train\n",
                "# x_test= data_zoom.x_test\n",
                "# y_test= data_zoom.y_test\n",
                "\n",
                "ax = plt.subplot(1, 1 , 1)\n",
                "plt.imshow(data_zoom.x_train[0].reshape(size_final,size_final), cmap='gray_r')\n",
                "ax.get_xaxis().set_visible(False)\n",
                "ax.get_yaxis().set_visible(False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [],
            "source": [
                "# data_zoom.x_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "BIT_WIDTH = 8\n",
                "EPOCHS = 10\n",
                "BATCH_SIZE = 32\n",
                "MODEL_NAME = 'model'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# model class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: \"model_7\"\n",
                        "_________________________________________________________________\n",
                        " Layer (type)                Output Shape              Param #   \n",
                        "=================================================================\n",
                        " input_8 (InputLayer)        [(None, 64)]              0         \n",
                        "                                                                 \n",
                        " dense_56 (Dense)            (None, 64)                4160      \n",
                        "                                                                 \n",
                        " dense_57 (Dense)            (None, 32)                2080      \n",
                        "                                                                 \n",
                        " dense_58 (Dense)            (None, 16)                528       \n",
                        "                                                                 \n",
                        " dense_59 (Dense)            (None, 2)                 34        \n",
                        "                                                                 \n",
                        " dense_60 (Dense)            (None, 16)                48        \n",
                        "                                                                 \n",
                        " dense_61 (Dense)            (None, 32)                544       \n",
                        "                                                                 \n",
                        " dense_62 (Dense)            (None, 32)                1056      \n",
                        "                                                                 \n",
                        " dense_63 (Dense)            (None, 64)                2112      \n",
                        "                                                                 \n",
                        "=================================================================\n",
                        "Total params: 10,562\n",
                        "Trainable params: 10,562\n",
                        "Non-trainable params: 0\n",
                        "_________________________________________________________________\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.layers import Input, Dense\n",
                "from tensorflow.keras.models import Model\n",
                "from callbacks import all_callbacks\n",
                "from matplotlib import pyplot as plt\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "# import MNIST_database as mnist\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "\n",
                "#Choose the final size of your image dataset\n",
                "size_final = 8\n",
                "# data_zoom = mnist.MNISTData(size_initial=20, size_final=size_final, color_depth=8, flat=True)\n",
                "\n",
                "class QAutoencoder:\n",
                "    def __init__(self, data: mnist.MNISTData, bit_width=8, model_name='model'):\n",
                "\n",
                "        self.x_train = data.x_train\n",
                "        self.y_train = data.y_train\n",
                "        self.x_test = data.x_test\n",
                "        self.y_test = data.y_test\n",
                "        self.input_min = np.min(data.x_train)\n",
                "        self.input_max = np.max(data.x_train)\n",
                "        self.input_shape = (data.x_train.shape[-1],)\n",
                "        self.BIT_WIDTH = bit_width\n",
                "        self.MODEL_NAME = model_name\n",
                "        self.model = self.autoencoder_generator()\n",
                "        self.history = None\n",
                "        self.loss = None\n",
                "        self.float_model_predictions = None\n",
                "\n",
                "        self.quantized_model_predictions = None\n",
                "        self.interpreter = None\n",
                "        self.quantized_tflite_model = None\n",
                "        self.input_details = None\n",
                "        self.output_details = None\n",
                "\n",
                "    def autoencoder_generator(self):\n",
                "\n",
                "        # Encoder\n",
                "        encoder_input = Input(shape=self.input_shape)\n",
                "        encoder_l1 = Dense(64, activation='relu')(encoder_input)\n",
                "        encoder_l2 = Dense(32, activation='relu')(encoder_l1)\n",
                "        encoder_l3 = Dense(16, activation='relu')(encoder_l2)\n",
                "        encoder_output = Dense(2, activation='relu')(encoder_l3)\n",
                "\n",
                "        # Decoder\n",
                "        decoder_l1 = Dense(16, activation='relu')(encoder_output)\n",
                "        decoder_l2 = Dense(32, activation='relu')(decoder_l1)\n",
                "        decoder_l3 = Dense(32, activation='relu')(decoder_l2)\n",
                "        # decoder_output = Dense(y_train.shape[-1], activation='sigmoid')(decoder_l3) # classifier\n",
                "        # decoder_output = Dense(\n",
                "        #     self.x_train.shape[-1], activation='sigmoid')(decoder_l3)  # autoencoder\n",
                "        decoder_output = Dense(self.x_train.shape[-1], activation='linear')(decoder_l3)\n",
                "\n",
                "        # Model\n",
                "        model = Model(inputs=encoder_input, outputs=decoder_output)\n",
                "        # refactor the code above to use the functional AP\n",
                "\n",
                "        model.compile(optimizer='adam', loss='mse')\n",
                "        # model.compile(optimizer='adam', loss='binary_crossentropy') # classifier\n",
                "        return model\n",
                "\n",
                "    def fit_data(self, batch_size=256, epochs=30):\n",
                "        \"\"\"Write the fit function for the autoencoder. \n",
                "        Storing the fit history in self.history to be able to plot the fitting scores.\"\"\"\n",
                "\n",
                "        callbacks = all_callbacks(stop_patience=1000,\n",
                "                                  lr_factor=0.5,\n",
                "                                  lr_patience=10,\n",
                "                                  lr_epsilon=0.000001,\n",
                "                                  # min_delta=0.000001,\n",
                "                                  lr_cooldown=2,\n",
                "                                  lr_minimum=0.0000001,\n",
                "                                  outputDir=f'model/QAE_model{self.BIT_WIDTH}bits/callbacks')\n",
                "        # callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
                "\n",
                "        self.history = self.model.fit(self.x_train, self.x_train,\n",
                "                                      validation_data=(\n",
                "                                          self.x_test, self.x_test),\n",
                "                                      batch_size=batch_size, epochs=epochs,\n",
                "                                      shuffle=True, callbacks=callbacks.callbacks)\n",
                "        # self.model = strip_pruning(self.model)\n",
                "        self.model.save(\n",
                "            # f'model/QAE_model{self.BIT_WIDTH}bits/KERAS_check_best_model.h5')\n",
                "            f'model/QAE_model{self.BIT_WIDTH}bits/KERAS_check_best_model.h5')\n",
                "        self.history = self.history.history\n",
                "        self.loss = self.model.evaluate(self.x_test, self.x_test, verbose=0)\n",
                "        self.convert_to_tflite()\n",
                "\n",
                "    def plot_float_model(self, n=6):\n",
                "        \"\"\"Plot the float model\"\"\"\n",
                "        \n",
                "        plt.figure(figsize=(10, 3))\n",
                "        self.float_model_predictions = self.model.predict(self.x_test)\n",
                "        self._extracted_from_plot_quantized_model_8(\n",
                "            n,\n",
                "            self.float_model_predictions,\n",
                "            './images/QAE/reconstructed images {model_name}.png',\n",
                "        )\n",
                "\n",
                "    # def representative_dataset(self):\n",
                "    #     for data in self.x_train:\n",
                "    #         # yield [np.array([data], dtype=np.float32)]\n",
                "    #         yield [np.array([data * (2 ** (self.BIT_WIDTH - 1))], dtype=np.float32)]\n",
                "\n",
                "    def representative_dataset(self):\n",
                "        for data in self.x_train:\n",
                "            # Scale the data using min and max values\n",
                "            scaled_data = (data - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))\n",
                "            yield [np.array([scaled_data], dtype=np.float32)]\n",
                "\n",
                "    def convert_to_tflite(self):\n",
                "\n",
                "        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n",
                "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "        converter.representative_dataset = self.representative_dataset\n",
                "        converter.target_spec.supported_ops = [\n",
                "            tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
                "        converter.inference_input_type = tf.int8\n",
                "        converter.inference_output_type = tf.int8\n",
                "\n",
                "        self.quantized_tflite_model = converter.convert()\n",
                "        # Save the quantized model\n",
                "        with open(f'quantized_model.tflite', 'wb') as f:\n",
                "            f.write(self.quantized_tflite_model)\n",
                "\n",
                "        # Load the quantized model\n",
                "        self.interpreter = tf.lite.Interpreter(\n",
                "            model_path='quantized_model.tflite')\n",
                "        self.interpreter.allocate_tensors()\n",
                "\n",
                "        # Get input and output details\n",
                "        self.input_details = self.interpreter.get_input_details()\n",
                "        self.output_details = self.interpreter.get_output_details()\n",
                "        self.quantized_predictions()\n",
                "\n",
                "    def quantized_predictions(self, n=6):\n",
                "        quantized_model_predictions = []\n",
                "\n",
                "        for i in range(n):\n",
                "            # Prepare input data\n",
                "            # input_data = np.array(\n",
                "            #     [self.x_test[i]*(2**(self.BIT_WIDTH-1))], dtype=np.int8)\n",
                "            input_data = np.array([(self.x_test[i] - self.input_min) / (self.input_max - self.input_min) * (2 ** (self.BIT_WIDTH - 1))], dtype=np.int8)  \n",
                "\n",
                "            self.interpreter.set_tensor(\n",
                "                self.input_details[0]['index'], input_data)\n",
                "            print(f\"input_data: {input_data}\")\n",
                "            # Run inference\n",
                "            self.interpreter.invoke()\n",
                "\n",
                "            # Get output\n",
                "            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
                "            output_data = self.interpreter.get_tensor(\n",
                "                # self.output_details[0]['index']) / (2 ** (self.BIT_WIDTH - 1))\n",
                "                self.output_details[0]['index'])\n",
                "            # output_data = output_data * (self.input_max - self.input_min) / (2 ** (self.BIT_WIDTH - 1)) + self.input_min\n",
                "            output_data = output_data / (2 ** (self.BIT_WIDTH - 1))\n",
                "            print(f\"output_data: {output_data}\")\n",
                "            quantized_model_predictions.append(output_data)\n",
                "        \n",
                "        self.quantized_model_predictions = quantized_model_predictions\n",
                "        # self.compute_mse()\n",
                "\n",
                "    def plot_quantized_model(self, n=6):\n",
                "\n",
                "        \n",
                "        plt.figure(figsize=(10, 3))\n",
                "        self._extracted_from_plot_quantized_model_8(\n",
                "            n,\n",
                "            self.quantized_model_predictions,\n",
                "            './images/QAE/reconstructed images{model_name}.png',\n",
                "        )\n",
                "\n",
                "    # TODO Rename this here and in `plot_float_model` and `plot_quantized_model`\n",
                "    def _extracted_from_plot_quantized_model_8(self, n, quantized_model_predictions, arg3):\n",
                "        img_size = int(np.sqrt(self.input_shape[0]))\n",
                "        for i in range(n):\n",
                "            ax = plt.subplot(2, n, i + 1)\n",
                "            self.plot_imgs(\n",
                "                self.x_test, i, img_size, ax\n",
                "            )\n",
                "            ax = plt.subplot(2, n, i + n + 1)\n",
                "            self.plot_imgs(\n",
                "                quantized_model_predictions, i, img_size, ax\n",
                "            )\n",
                "        plt.savefig(arg3.format(model_name=\" complete\"))\n",
                "        plt.show()\n",
                "\n",
                "    # TODO Rename this here and in `plot_float_model` and `plot_quantized_model`\n",
                "    def plot_imgs(self, arg0, i, img_size, ax):\n",
                "        plt.imshow(arg0[i].reshape(img_size, img_size), cmap='gray_r')\n",
                "        ax.get_xaxis().set_visible(False)\n",
                "        ax.get_yaxis().set_visible(False)\n",
                "\n",
                "    def compute_mse(self):\n",
                "        mse = mean_squared_error(self.float_model_predictions, self.quantized_model_predictions)\n",
                "        print(f'Mean Squared Error between floating point and quantized model predictions: {mse}')\n",
                "        self.mse = mse\n",
                "\n",
                "model_obj = QAutoencoder(data_zoom, bit_width=BIT_WIDTH, model_name=MODEL_NAME)\n",
                "# model.fit(x_train, y_train, epochs=10, batch_size=32) # classifier\n",
                "model_obj.model.summary()\n",
                "# model.fit(x_train, x_train, epochs=EPOCHS, batch_size=BATCH_SIZE) #autoencoder\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# fit data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/7\n",
                        "  1/235 [..............................] - ETA: 9s - loss: 0.0956WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "231/235 [============================>.] - ETA: 0s - loss: 0.0906\n",
                        "***callbacks***\n",
                        "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
                        "\n",
                        "Epoch 00001: val_loss improved from inf to 0.08370, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00001: val_loss improved from inf to 0.08370, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
                        "\n",
                        "Epoch 00001: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00001: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
                        "\n",
                        "***callbacks end***\n",
                        "\n",
                        "235/235 [==============================] - 5s 20ms/step - loss: 0.0905 - val_loss: 0.0837 - lr: 0.0010\n",
                        "Epoch 2/7\n",
                        "230/235 [============================>.] - ETA: 0s - loss: 0.0814\n",
                        "***callbacks***\n",
                        "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
                        "\n",
                        "Epoch 00002: val_loss improved from 0.08370 to 0.07932, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00002: val_loss improved from 0.08370 to 0.07932, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
                        "\n",
                        "Epoch 00002: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00002: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
                        "\n",
                        "***callbacks end***\n",
                        "\n",
                        "235/235 [==============================] - 4s 18ms/step - loss: 0.0814 - val_loss: 0.0793 - lr: 0.0010\n",
                        "Epoch 3/7\n",
                        "223/235 [===========================>..] - ETA: 0s - loss: 0.0788\n",
                        "***callbacks***\n",
                        "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
                        "\n",
                        "Epoch 00003: val_loss improved from 0.07932 to 0.07709, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00003: val_loss improved from 0.07932 to 0.07709, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
                        "\n",
                        "Epoch 00003: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00003: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
                        "\n",
                        "***callbacks end***\n",
                        "\n",
                        "235/235 [==============================] - 5s 20ms/step - loss: 0.0787 - val_loss: 0.0771 - lr: 0.0010\n",
                        "Epoch 4/7\n",
                        "224/235 [===========================>..] - ETA: 0s - loss: 0.0766\n",
                        "***callbacks***\n",
                        "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
                        "\n",
                        "Epoch 00004: val_loss improved from 0.07709 to 0.07505, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00004: val_loss improved from 0.07709 to 0.07505, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
                        "\n",
                        "Epoch 00004: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00004: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
                        "\n",
                        "***callbacks end***\n",
                        "\n",
                        "235/235 [==============================] - 4s 16ms/step - loss: 0.0766 - val_loss: 0.0750 - lr: 0.0010\n",
                        "Epoch 5/7\n",
                        "223/235 [===========================>..] - ETA: 0s - loss: 0.0747\n",
                        "***callbacks***\n",
                        "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
                        "\n",
                        "Epoch 00005: val_loss improved from 0.07505 to 0.07332, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00005: val_loss improved from 0.07505 to 0.07332, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
                        "\n",
                        "Epoch 00005: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00005: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
                        "\n",
                        "***callbacks end***\n",
                        "\n",
                        "235/235 [==============================] - 4s 19ms/step - loss: 0.0747 - val_loss: 0.0733 - lr: 0.0010\n",
                        "Epoch 6/7\n",
                        "235/235 [==============================] - ETA: 0s - loss: 0.0736\n",
                        "***callbacks***\n",
                        "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
                        "\n",
                        "Epoch 00006: val_loss improved from 0.07332 to 0.07258, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00006: val_loss improved from 0.07332 to 0.07258, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
                        "\n",
                        "Epoch 00006: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00006: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
                        "\n",
                        "***callbacks end***\n",
                        "\n",
                        "235/235 [==============================] - 4s 18ms/step - loss: 0.0736 - val_loss: 0.0726 - lr: 0.0010\n",
                        "Epoch 7/7\n",
                        "233/235 [============================>.] - ETA: 0s - loss: 0.0727\n",
                        "***callbacks***\n",
                        "saving losses to model/QAE_model8bits/callbacks\\losses.log\n",
                        "\n",
                        "Epoch 00007: val_loss improved from 0.07258 to 0.07158, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00007: val_loss improved from 0.07258 to 0.07158, saving model to model/QAE_model8bits/callbacks\\KERAS_check_best_model_weights.h5\n",
                        "\n",
                        "Epoch 00007: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/callbacks\\KERAS_check_model_last.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 00007: saving model to model/QAE_model8bits/callbacks\\KERAS_check_model_last_weights.h5\n",
                        "\n",
                        "***callbacks end***\n",
                        "\n",
                        "235/235 [==============================] - 4s 18ms/step - loss: 0.0727 - val_loss: 0.0716 - lr: 0.0010\n",
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: model/QAE_model8bits/KERAS_check_best_model.model\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmp7wqsa8m0\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmp7wqsa8m0\\assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "input_data: [[   0    0    0    0    0    0    0    0    0   56   34    0    0    0\n",
                        "     0    0    0   24   31   89   92   71  118    0    0    0    0    0\n",
                        "     0   76   18    0    0    0    0    0    0  127    1    0    0    0\n",
                        "     0    0   92   15    0    0    0    0    0   13  102    1    0    0\n",
                        "     0    0    0 -128    0    0    0    0]]\n",
                        "output_data: [[-0.4609375 -0.4921875 -0.421875  -0.390625  -0.2578125 -0.2109375\n",
                        "  -0.421875  -0.4609375 -0.4921875 -0.4609375 -0.2734375  0.1171875\n",
                        "   0.21875    0.1015625 -0.1328125 -0.4375    -0.5       -0.59375\n",
                        "  -0.1171875 -0.203125  -0.4453125 -0.3515625 -0.1015625 -0.390625\n",
                        "  -0.484375  -0.359375   0.234375  -0.1875    -0.265625  -0.1171875\n",
                        "  -0.0078125 -0.4453125 -0.40625   -0.1796875  0.3203125  0.046875\n",
                        "  -0.109375   0.015625   0.0703125 -0.4140625 -0.4609375 -0.109375\n",
                        "   0.03125   -0.2890625 -0.265625   0.1484375  0.046875  -0.515625\n",
                        "  -0.40625   -0.0859375  0.015625  -0.109375   0.125      0.109375\n",
                        "  -0.359375  -0.46875   -0.4140625 -0.390625  -0.2265625 -0.1328125\n",
                        "  -0.1640625 -0.359375  -0.5       -0.4765625]]\n",
                        "input_data: [[   0    2  111  126  126    0    0    0    0   20   83    0  107    9\n",
                        "     0    0    0    1    0   23 -128    0    0    0    0    0    0 -128\n",
                        "     7    0    0    0    0    0   72   88    0    0    0    0    0    0\n",
                        "  -128    0    2    1    2    0    0    0  126 -128 -128   97 -128  124\n",
                        "     0    0    0    0    0    0    0    0]]\n",
                        "output_data: [[-0.4609375 -0.484375  -0.390625  -0.328125  -0.1796875 -0.1328125\n",
                        "  -0.4296875 -0.4375    -0.5078125 -0.5234375 -0.359375   0.015625\n",
                        "   0.140625  -0.046875  -0.203125  -0.46875   -0.484375  -0.6640625\n",
                        "  -0.2421875 -0.1015625 -0.3984375 -0.5       -0.15625   -0.375\n",
                        "  -0.4453125 -0.4296875  0.28125   -0.1796875 -0.359375  -0.265625\n",
                        "   0.0859375 -0.4765625 -0.3828125 -0.1328125  0.3203125 -0.140625\n",
                        "  -0.2734375 -0.1328125  0.2421875 -0.421875  -0.4296875 -0.0703125\n",
                        "   0.1484375 -0.3046875 -0.3359375  0.140625   0.125     -0.53125\n",
                        "  -0.40625   -0.1171875  0.0859375  0.015625   0.1640625  0.078125\n",
                        "  -0.390625  -0.4609375 -0.4375    -0.40625   -0.2890625 -0.265625\n",
                        "  -0.359375  -0.46875   -0.5       -0.453125 ]]\n",
                        "input_data: [[   0    0    0    0    1   99    1    0    0    0    0    0    8   32\n",
                        "     0    0    0    0    0    0  104    0    0    0    0    0    0    0\n",
                        "  -128    0    0    0    0    0    0    7   60    0    0    0    0    0\n",
                        "     1   81    7    0    0    0    0    0    0 -128    0    0    0    0\n",
                        "     0    0    0   76    0    0    0    0]]\n",
                        "output_data: [[-0.4609375 -0.4765625 -0.390625  -0.34375   -0.1875    -0.15625\n",
                        "  -0.4296875 -0.4375    -0.5078125 -0.5234375 -0.3671875 -0.0078125\n",
                        "   0.1171875 -0.0625    -0.203125  -0.46875   -0.484375  -0.65625\n",
                        "  -0.2578125 -0.1171875 -0.40625   -0.5       -0.171875  -0.3828125\n",
                        "  -0.4453125 -0.4375     0.2421875 -0.1796875 -0.359375  -0.2890625\n",
                        "   0.0546875 -0.46875   -0.3828125 -0.1484375  0.265625  -0.1640625\n",
                        "  -0.2890625 -0.1640625  0.203125  -0.421875  -0.4296875 -0.0859375\n",
                        "   0.109375  -0.3203125 -0.359375   0.1015625  0.1015625 -0.53125\n",
                        "  -0.40625   -0.1328125  0.0625    -0.0078125  0.1328125  0.0546875\n",
                        "  -0.390625  -0.46875   -0.4375    -0.3984375 -0.2890625 -0.265625\n",
                        "  -0.3515625 -0.46875   -0.5       -0.453125 ]]\n",
                        "input_data: [[   0    0    0   12  110    0    0    0    0    0   13  126 -128   22\n",
                        "     0    0    0    1  125  126   90  114    0    0    0    3  125    0\n",
                        "     0   18 -128    0    0   40   70    0    0   18 -128    0    0   78\n",
                        "    73    0   39 -128   79    0    0   22 -128  123  123  112    0    0\n",
                        "     0    0   11  124   36    0    0    0]]\n",
                        "output_data: [[-0.4609375 -0.484375  -0.3828125 -0.3203125 -0.1484375 -0.109375\n",
                        "  -0.421875  -0.4375    -0.5078125 -0.53125   -0.3515625  0.046875\n",
                        "   0.1953125 -0.015625  -0.1796875 -0.4609375 -0.484375  -0.6796875\n",
                        "  -0.2265625 -0.0625    -0.390625  -0.5078125 -0.1328125 -0.3671875\n",
                        "  -0.4453125 -0.4375     0.34375   -0.1484375 -0.34375   -0.25\n",
                        "   0.125     -0.46875   -0.375     -0.109375   0.375     -0.125\n",
                        "  -0.2578125 -0.109375   0.3046875 -0.421875  -0.4296875 -0.046875\n",
                        "   0.1953125 -0.2890625 -0.3359375  0.1875     0.171875  -0.5390625\n",
                        "  -0.3984375 -0.09375    0.1328125  0.0625     0.21875    0.1328125\n",
                        "  -0.390625  -0.4609375 -0.4296875 -0.40625   -0.28125   -0.25\n",
                        "  -0.3515625 -0.46875   -0.5       -0.453125 ]]\n",
                        "input_data: [[   0    0    0    0    0    0    0    0    0    0   31    0    0    0\n",
                        "    80    0    0    0  120    1    0    0   97    0    0   61   16    0\n",
                        "     0   78    9    0    0   78    0    0    0 -128    0    0    0    0\n",
                        "    79   89   45 -128    4    0    0    0    0    0    0 -128    3    0\n",
                        "     0    0    0    0    0   85    1    0]]\n",
                        "output_data: [[-0.4765625 -0.4765625 -0.4140625 -0.375     -0.171875  -0.1640625\n",
                        "  -0.4375    -0.4609375 -0.5       -0.53125   -0.4296875 -0.03125\n",
                        "   0.140625  -0.0078125 -0.140625  -0.4765625 -0.484375  -0.6640625\n",
                        "  -0.3125    -0.140625  -0.3828125 -0.546875  -0.109375  -0.328125\n",
                        "  -0.4453125 -0.4140625  0.125     -0.171875  -0.3828125 -0.5546875\n",
                        "   0.0546875 -0.390625  -0.421875  -0.1171875  0.03125   -0.3359375\n",
                        "  -0.46875   -0.4609375  0.1875    -0.3671875 -0.390625   0.03125\n",
                        "  -0.0078125 -0.4453125 -0.53125   -0.0546875  0.1328125 -0.5078125\n",
                        "  -0.390625   0.0078125  0.171875  -0.0234375  0.1015625  0.03125\n",
                        "  -0.375     -0.4765625 -0.4375    -0.328125  -0.171875  -0.1640625\n",
                        "  -0.359375  -0.53125   -0.5       -0.4609375]]\n",
                        "input_data: [[   0    0    0    0    0    0    0    0    0    0    0    0   17  119\n",
                        "     2    0    0    0    0    0  120   45    0    0    0    0    0    1\n",
                        "  -128    0    0    0    0    0    0   39  122    2    0    0    0    0\n",
                        "     2  120   19    0    0    0    0    0    0 -128    0    0    0    0\n",
                        "     0    0    0  126    0    0    0    0]]\n",
                        "output_data: [[-0.46875   -0.5       -0.4375    -0.4765625 -0.390625  -0.3515625\n",
                        "  -0.421875  -0.4765625 -0.484375  -0.3984375 -0.1484375  0.1875\n",
                        "   0.265625   0.265625  -0.03125   -0.40625   -0.5       -0.46875\n",
                        "  -0.0390625 -0.34375   -0.4609375 -0.1875    -0.1015625 -0.40625\n",
                        "  -0.4921875 -0.2890625  0.0234375 -0.1796875 -0.1015625 -0.0625\n",
                        "  -0.2265625 -0.3984375 -0.453125  -0.3125     0.0859375  0.1484375\n",
                        "  -0.0078125  0.015625  -0.28125   -0.421875  -0.4609375 -0.21875\n",
                        "  -0.21875   -0.3671875 -0.28125    0.        -0.15625   -0.5\n",
                        "  -0.390625  -0.1171875 -0.078125  -0.328125  -0.0234375  0.0859375\n",
                        "  -0.3125    -0.4921875 -0.40625   -0.3125    -0.078125   0.078125\n",
                        "   0.0703125 -0.265625  -0.484375  -0.484375 ]]\n"
                    ]
                }
            ],
            "source": [
                "model_obj.fit_data(epochs=7)  # batch_size=BATCH_SIZE, epochs=EPOCHS)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# plot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": [
                "# def plot_quantized_model(obj, n=6):\n",
                "#     quantized_model_predictions = []\n",
                "#     # test_imgs = x_train\n",
                "#     test_imgs = obj.x_test\n",
                "\n",
                "#     for i in range(n):\n",
                "#         # Prepare input data\n",
                "#         input_data = np.array(\n",
                "#             [test_imgs[i]*(2**(obj.BIT_WIDTH-1))], dtype=np.int8)\n",
                "#         obj.interpreter.set_tensor(\n",
                "#             obj.input_details[0]['index'], input_data)\n",
                "#         # print(f\"input_data: {input_data}\")\n",
                "#         # Run inference\n",
                "#         obj.interpreter.invoke()\n",
                "\n",
                "#         # Get output\n",
                "#         # output_data = interpreter.get_tensor(output_details[0]['index'])\n",
                "#         output_data = obj.interpreter.get_tensor(\n",
                "#             # obj.output_details[0]['index']) / (2 ** (obj.BIT_WIDTH - 1))\n",
                "#             obj.output_details[0]['index'])\n",
                "#         print(f\"output_data: {output_data}\")\n",
                "#         quantized_model_predictions.append(output_data)\n",
                "#         print(\"------------------ \\n\")\n",
                "\n",
                "#     plt.figure(figsize=(10, 3))\n",
                "#     obj._extracted_from_plot_quantized_model_8(\n",
                "#         n,\n",
                "#         test_imgs,\n",
                "#         quantized_model_predictions,\n",
                "#         './images/QAE/reconstructed images{model_name}.png',\n",
                "#     )\n",
                "\n",
                "# # TODO Rename this here and in `plot_float_model` and `plot_quantized_model`\n",
                "# def _extracted_from_plot_quantized_model_8(obj, n, test_imgs, quantized_model_predictions, arg3):\n",
                "#     img_size = int(np.sqrt(obj.input_shape[0]))\n",
                "#     for i in range(n):\n",
                "#         ax = plt.subplot(2, n, i + 1)\n",
                "#         obj.plot_imgs(\n",
                "#             test_imgs, i, img_size, ax\n",
                "#         )\n",
                "#         ax = plt.subplot(2, n, i + n + 1)\n",
                "#         obj.plot_imgs(\n",
                "#             quantized_model_predictions, i, img_size, ax\n",
                "#         )\n",
                "#     plt.savefig(arg3.format(model_name=\" complete\"))\n",
                "#     plt.show()\n",
                "\n",
                "# # TODO Rename this here and in `plot_float_model` and `plot_quantized_model`\n",
                "# def plot_imgs(obj, arg0, i, img_size, ax):\n",
                "#     plt.imshow(arg0[i].reshape(img_size, img_size), cmap='gray_r')\n",
                "#     ax.get_xaxis().set_visible(False)\n",
                "#     ax.get_yaxis().set_visible(False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 1000x300 with 12 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "model_obj.plot_float_model()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxUAAAD7CAYAAAAcu7llAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWq0lEQVR4nO3ba4wcVPk/8Ge2u3RLd1tiFUPZxRKDgFxiES+8EI2BBFAUFIwBYxCFqJigJihqMPoG36lBeEEMYBrFS7xx8Z6AgNcYCFViuRWQlqX0Iu3O3ne783tBqv7278//POfsTHfx83l9vnvOzDxzZr87baPVarUCAACgUM/BPgAAALC8KRUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQpbedRfPz8zEyMhKDg4PRaDQ6fSYKtFqtaDabsX79+ujp6XxXNBNLn5lgITPBQt2cCfOw9LkjWCgzE22VipGRkRgeHl6Uw9FZ27Zti6GhoY7vYyaWDzPBQmaChboxE+Zh+XBHsFA7M9FWqRgcHPzHD1yzZk39yVh0o6OjMTw8/I/XqtNKZ2Jubi6918aNG9OZp59+Op0p/SvJ3r17i3KdtlxmYimbn58vyp155pnpzK9+9at0JvuXRDPx/3ruuefSmZLXd/PmzelMN/5y282ZWA7z8N/OHcFCmZloq1QcuNjWrFnjRV/iuvX1YelMlJSKkq9gS56H0uduqb8nlvpMLGWlpaK3t62r9X8pec5K/3mCmfiniYmJdKbkeS95/N385yDd2Gs5zAMvcEewUDsz4T9qAwAAVZQKAACgilIBAABUUSoAAIAqSgUAAFBFqQAAAKooFQAAQBWlAgAAqKJUAAAAVZQKAACgSm8nf/hXv/rVdOaEE05IZ84888x0hoPjscceS2fe+MY3pjNPPvlkOnP88cenMxERe/bsSWfWrVtXtBfdNTo6WpT70pe+lM709PgbT41nnnmmKDc8PJzOtFqtdOahhx5KZ0466aR0hhfce++96czpp5/egZMsnksuuSSd+cY3vrHo5+A/27p1a1Hu1FNPTWd27tyZzvT19aUz7fIpBgAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACq9Hbyh7/lLW9JZ/bt25fOvOMd70hnpqen05mIiFe/+tXpzKZNm9KZPXv2pDPLwbHHHpvO3HrrrenM5s2b05mHH344nYmIWLduXVGO7mq1WulMyR0WEfHggw8W5XjB/Px8OrNhw4aivRqNRjpzzjnnpDNnn312OrN9+/Z0hhece+656czzzz+fzvT0dO9vs0888UTX9qLcKaecUpS74YYb0pm+vr6ivTrFNxUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoEpvJ3/4a17zmk7++H9485vf3JV9IiI+/OEPpzN//OMfO3CS5amnpzs9tmT29u7du+jnYOloNBrpzObNmztwEv5/5ubm0pmS1zciYv/+/elMq9VKZx5//PF0pvQxlZxvKbv22mvTmZL3brc+nyIiPvjBD6YzP/rRjzpwEv6T+fn5dGZ0dLRor/e9731FuaXENxUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoErvwT7AcnPjjTemMzfccEMHTvLf48orr0xn3vnOd6Yza9euTWdYPkZHR9OZa6+9tmivVquVzjQajaK9Xox+8pOfpDMf+9jHOnCSf89r1V1bt25NZzZs2LD4B1lEN998czpz0003deAk/Cd33nlnOrNq1aqivWZnZ9OZvr6+or06xTcVAABAFaUCAACoolQAAABVlAoAAKCKUgEAAFRRKgAAgCpKBQAAUEWpAAAAqigVAABAFaUCAACoolQAAABVlAoAAKCKUgEAAFTpPdgHWAytViudOeuss4r2euqpp9KZFStWFO3FC6677rp0pmQmZmdn05mIiJ6efDcvOV9v74vi7boo5ufn05krrrginbn++uvTmYiIRqNRlOMFd9xxRzozMDDQgZP8e7fddls6c95556Uzl1xySTrzYnTTTTelM5deemk6s3v37nTmve99bzpTampqKp3p7+/vwEmWp7m5uXTm/e9/fzrz4IMPpjMREX19fUW5pcQ3FQAAQBWlAgAAqKJUAAAAVZQKAACgilIBAABUUSoAAIAqSgUAAFBFqQAAAKooFQAAQBWlAgAAqKJUAAAAVZQKAACgSu/BPsBiePbZZ9OZ3bt3F+01NDRUlKNcT0+++zYajQ6c5N9705velM7cd9996cwhhxySWt9qtdJ7LBclM/HNb34zndm0aVM6Q71PfOIT6czJJ59ctNfXv/71olzWUUcdlc7ccsstHTjJf4ebb745nSm5M0s/ay6++OKu7cULenvzv/Lu27cvnXnVq16VzrxY+KYCAACoolQAAABVlAoAAKCKUgEAAFRRKgAAgCpKBQAAUEWpAAAAqigVAABAFaUCAACoolQAAABVlAoAAKCKUgEAAFTpPdgHWAxHHnlkOtNqtTpwEjph//79B/sIy9Lo6GisXbv2YB9jybj88ssP9hFo00knnZTO7Nixo2ivo446Kp257bbb0pmzzjornaG7Go1G1/a68MIL05m+vr4OnIT/pKfH394zPFsAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFV621nUarUiImJ0dLSjh+mmF9Njifjn4znwWnXai3EmXmzMxP82MzOTzpQ+lkajUZTrtBfzTDSbzaJcyXMxPj6ezizV90U3Z2Kp3xHdNDs7m86UPG89Pbm/Hb+Y74gSJc/DUn0spTIz0Wi1sWr79u0xPDxcfzI6btu2bTE0NNTxfczE8mEmWMhMsFA3ZsI8LB/uCBZqZybaKhXz8/MxMjISg4ODS/YvcP/tWq1WNJvNWL9+ffovEyXMxNJnJljITLBQN2fCPCx97ggWysxEW6UCAADg/+I/agMAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFV621k0Pz8fIyMjMTg4GI1Go9NnokCr1Ypmsxnr16+Pnp7Od0UzsfSZCRYyEyzUzZkwD0ufO4KFMjPRVqkYGRmJ4eHhRTkcnbVt27YYGhrq+D5mYvkwEyxkJlioGzNhHpYPdwQLtTMTbZWKwcHBiIi49957Y2BgoO0D9Pf3t732gD179qQz09PT6czjjz+ezkREjI6OpjOnnXZaOrN69erU+rGxsTj99NP/8Vp12oF97rzzztRZDznkkPReO3bsSGf27duXzpTOxNjYWDpzzjnnpDPZvxqNj4/H+eef3/WZuP3221Mz0dfXl95r586d6UxERLPZTGe2bNmSzkxNTaUzZ511VjoTkZuL8fHxePe73931mbjjjjuW5D0RUXanP/roo+nMxMREOlM6E729bX20R0R374kDe9x2223pz7iskt8lSl6j3//+9+lMRMT+/fvTmXPPPTedWblyZWr9+Ph4XHDBBV2/Ix599NHUnoceemh6r5KZyPzOe8D27dvTmYiy3yUOP/zwdOYlL3lJan2z2YxXvvKVbb0+bd08B76SGhgYSD3Bq1atanvtASUFIXOBHlBytoiI2dnZdKZkKEsv3G59fXhgn9WrV6ceX8kvCyXPRcnrVFKCS/cqeUylX0Uv9ZkoKRUll29E2Qd59kM54oWvi7NK3/Mlc7HUZ6Jb90RExNzcXDpTMhMls1f6mEo+E7sxE/86D50uFZOTkx39+QeUzGpE9+ahZFYjun9HDA4Oxpo1a9rOlZSKmZmZdKakXHWrkJXulXme/1U7M+E/agMAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoEpvZnF/f3+sWrWq7fWTk5PpA23ZsiWdue6669KZ7du3pzMREUcffXQ6s2vXrnTmwgsvTK2fnp5O77EYBgYGYmBgoO31vb2pkYuIiKmpqXTm29/+djrz5JNPpjMRERs3bkxnfvvb36YzZ555Zmr93Nxceo/FMDg4mJqJvr6+9B5/+9vf0pmIiE2bNqUzW7duTWde+9rXpjP33HNPOhMRcfbZZ7e99mDNRPaeKJmJ0juw5K54/PHH05mSmSi5JyIizjjjjLbXltzJtQ455JBYuXJl2+sPPfTQ9B6jo6PpzJe//OV05v77709nIiJOOOGEdKbks/Dyyy9PrS957y2Gvr6+1N4ln9ff//7305mvfOUr6cyzzz6bzpS69NJL05nPf/7zqfXNZrPttb6pAAAAqigVAABAFaUCAACoolQAAABVlAoAAKCKUgEAAFRRKgAAgCpKBQAAUEWpAAAAqigVAABAFaUCAACoolQAAABVejOLe3p6oqen/R5yzDHHpA90yy23pDMXXXRROnPGGWekMxERIyMj6cwvf/nLdGZsbCy1fnx8PL3HYpiZmYmZmZm21x933HHpPX72s5+lM0cddVQ6c8UVV6QzERFHHnlkOnPrrbemM9nZm5iYSO+xGLIzcfzxx6f3+OlPf5rORJTNxZVXXpnOHHHEEelMyUxEROzYsaPttS/mmSi5JyIi1q9fn8589KMfTWeGh4fTmW9961vpTETEc8891/bagzETs7OzMTs72/b6Y489Nr3HXXfdlc5s2LAhnSmZhYiIjRs3pjPXXHNNOvPQQw+l1k9OTqb3WAx9fX3R19fX9vqXv/zl6T0ee+yxdOb2229PZ0rmNaLs/f70008X7dUpvqkAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFV6M4vn5uZibm6u7fV33313+kC33HJLOvOpT30qnbn//vvTmYiIv/71r+nMM888k86sXbs2tX7FihXpPRbD5ORk9PS0301/85vfpPf49Kc/nc585jOfSWcmJibSmYiI++67L52555570pkLLrggtX5sbCy9x2KYmJiIRqPR9vqSmbjqqqvSmdJcs9lMZ5566ql05te//nU6ExHxrne9q+21B2smpqamUnfU7373u/QepTNx9dVXpzPj4+PpTMl7viQTEXH++ee3vfZgzET2jrjzzjvTe3zuc59LZy6++OJ0plTJZ/YDDzyQzmQ/N0o/B2vNz8/H/Px82+tLPjd27dqVzvzhD39IZ374wx+mMxFl78XXv/716Uzmec6u900FAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKjSm1m8a9eumJycbHv9Aw88kD7Qjh070pm77rornTnxxBPTmYiIrVu3pjP79u1LZzLPc8n6xbJ3796YmZlpe/3dd9+d3qPksY2Pj6czO3fuTGciymbi4YcfTmdGR0dT60ueg8Xw/PPPx/T0dNvrS96/mZn7V3Nzc+nMnj170pmSmdiyZUs6ExHRbDbbXnuwZqIb90Rm5v5VySzt2rUrnXniiSfSmYceeiidici9zhMTE0V71Jieno7e3vZ//fjOd76T3mN4eDid2bhxYzqzfv36dCYiYtu2benMI488ks5k3xel76NazWYzGo1G2+v7+/vTe+zevTuduf7669OZU089NZ2JKPtdZ926denMypUrU+szd6RvKgAAgCpKBQAAUEWpAAAAqigVAABAFaUCAACoolQAAABVlAoAAKCKUgEAAFRRKgAAgCpKBQAAUEWpAAAAqigVAABAFaUCAACo0ptZvHbt2hgYGGh7/e7du9MHOu+889KZiy66KJ2Znp5OZyIifv7zn6czQ0ND6czw8HBqfbPZTO+xGAYGBmL16tVtr9+7d296jxNPPDGdOeGEE9KZ/v7+dCYi4he/+EU6k319I/JzdLBmYnBwsOMzUfL6luYyj+WAr33ta+lMyUxERLziFa9oe+1yuSf27duX3qN0Jo477rh0pmQmrr/++nRmw4YN6UxE7q44GDMxOTkZjUaj7fV/+ctf0nuUvEZHH310OrNu3bp0JiLi4x//eDpz2GGHpTPZ98XY2Fh6j8XQ09MTPT3t/537qaeeSu9x2WWXpTOnnHJKOrNt27Z0JiJi06ZN6cyWLVvSmbPPPju1fnR0tO21vqkAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFV6M4v37dsX+/fvb3v99PR0+kB9fX3pzAMPPJDO3HXXXelMRMTq1avTmU9+8pPpzOjoaGp9s9lM77EYpqeno7e3/TEaGBhI73HYYYelMz/4wQ/SmUceeSSdiYg4/PDD05nPfvaz6czf//731PqxsbH0HoshOxODg4PpPUoyERHf/e5305mSuXjZy16WzlxzzTXpTERuLg7WTExNTcWKFSvaXl9yz65ZsyadiYj43ve+l848+uij6cxLX/rSdOYLX/hCOhMRsWfPnrbXHqyZyOjv709n/vSnP6UzX/ziF9OZbn5u3HjjjenMxMREav3k5GR6j8Wwc+fO1N5PPPFEeo/s71UREcccc0w6U3KnRERs3749nfnIRz6SzszPz3dsvW8qAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQRakAAACqKBUAAECV3szinp6e6Olpv4e85z3vSR/oz3/+czrzzDPPpDPnn39+OhMR8YY3vCGdOfbYY9OZmZmZ1PoVK1ak91gMq1atilWrVrW9/m1ve1t6j5NOOimdyT5/EREf+MAH0pmIiOHh4XSmv78/nRkbG0utn5+fT++xGLIzcc4556T3OPHEE9OZiIjp6el05pJLLklnNmzYkM6UzEREbi72799ftEet/v7+1ON7+9vfnt7j5JNPTmciIqamptKZI444Ip0ZGhpKZ1auXJnORERMTEy0vbbVahXtUWNwcDBWr17d9vrLLrssvcfmzZvTmZL3x4c+9KF0JiLida97XTqTeV0PyH5uzM7OpvdYDH19fdHX19f2+re+9a3pPX784x+nM1dffXU6U/r72FVXXZXOnHbaaelMds4z631TAQAAVFEqAACAKkoFAABQRakAAACqKBUAAEAVpQIAAKiiVAAAAFWUCgAAoIpSAQAAVFEqAACAKkoFAABQpbedRa1WKyIixsfHUz98dnY2faDJycl0Znp6Op1pNBrpTET+OYiIaDab6czMzExq/djYWET887XqtNKZKHl9SzLZ5y8iYmJiIp2J+OdznzE3N9fxfQ68Nkt9JkreUyUzEVF2V5TMRbdmIiL3/C2XmSh5/5bOxNTUVDrTrZko+QyNyJ2vmzNxYI/s89etz4D9+/enM6VzVzIPJXNXeh93+44o/XzLKLn/S+7l0ueu5PUdHR0t2ivjwO+w7TyuRquNVdu3b4/h4eH6k9Fx27Zti6GhoY7vYyaWDzPBQmaChboxE+Zh+XBHsFA7M9FWqZifn4+RkZEYHBws/gs/ndVqtaLZbMb69eujp6fz/6rNTCx9ZoKFzAQLdXMmzMPS545gocxMtFUqAAAA/i/+ozYAAFBFqQAAAKooFQAAQBWlAgAAqKJUAAAAVZQKAACgilIBAABU+R+dBWWUT704IAAAAABJRU5ErkJggg==",
                        "text/plain": [
                            "<Figure size 1000x300 with 12 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# obj = model_obj\n",
                "# plot_quantized_model(obj, n=6)\n",
                "model_obj.plot_quantized_model(n=6)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([-0.02150068,  0.0156076 ,  0.02179566,  0.07375401,  0.02924037,\n",
                            "       -0.03326507, -0.02505459,  0.00884648,  0.01045914,  0.07307841,\n",
                            "        0.15243047,  0.2476467 ,  0.2417716 ,  0.09657636,  0.04357026,\n",
                            "        0.00994353,  0.05986412,  0.251407  ,  0.42270425,  0.42253816,\n",
                            "        0.4069883 ,  0.45535773,  0.15133089,  0.03943221,  0.04784808,\n",
                            "        0.18330134,  0.25623325,  0.13010374,  0.27085775,  0.4764258 ,\n",
                            "        0.04859004,  0.00758731,  0.00995792,  0.12882039,  0.20396416,\n",
                            "        0.27546817,  0.5731055 ,  0.46107057, -0.00384685,  0.03705094,\n",
                            "        0.00364595,  0.00687055,  0.02548716,  0.11691193,  0.5974051 ,\n",
                            "        0.19131678, -0.02907322,  0.0494942 , -0.01148535, -0.07801425,\n",
                            "       -0.04677773,  0.19116828,  0.48137936,  0.11250585,  0.0366061 ,\n",
                            "       -0.00138363, -0.0136549 , -0.00297997,  0.00766612,  0.20607652,\n",
                            "        0.37499186,  0.12558827,  0.05858274,  0.02201036], dtype=float32)"
                        ]
                    },
                    "execution_count": 69,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_obj.float_model_predictions[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[-0.4609375, -0.4921875, -0.421875 , -0.390625 , -0.2578125,\n",
                            "        -0.2109375, -0.421875 , -0.4609375, -0.4921875, -0.4609375,\n",
                            "        -0.2734375,  0.1171875,  0.21875  ,  0.1015625, -0.1328125,\n",
                            "        -0.4375   , -0.5      , -0.59375  , -0.1171875, -0.203125 ,\n",
                            "        -0.4453125, -0.3515625, -0.1015625, -0.390625 , -0.484375 ,\n",
                            "        -0.359375 ,  0.234375 , -0.1875   , -0.265625 , -0.1171875,\n",
                            "        -0.0078125, -0.4453125, -0.40625  , -0.1796875,  0.3203125,\n",
                            "         0.046875 , -0.109375 ,  0.015625 ,  0.0703125, -0.4140625,\n",
                            "        -0.4609375, -0.109375 ,  0.03125  , -0.2890625, -0.265625 ,\n",
                            "         0.1484375,  0.046875 , -0.515625 , -0.40625  , -0.0859375,\n",
                            "         0.015625 , -0.109375 ,  0.125    ,  0.109375 , -0.359375 ,\n",
                            "        -0.46875  , -0.4140625, -0.390625 , -0.2265625, -0.1328125,\n",
                            "        -0.1640625, -0.359375 , -0.5      , -0.4765625]])"
                        ]
                    },
                    "execution_count": 70,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_obj.quantized_model_predictions[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(64,)"
                        ]
                    },
                    "execution_count": 90,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_obj.x_test[0].shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# new load"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmpx6jk1rd8\\assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: C:\\Users\\luisa\\AppData\\Local\\Temp\\tmpx6jk1rd8\\assets\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "import tensorflow.lite as tflite\n",
                "\n",
                "class Model:\n",
                "    def __init__(self, data: mnist.MNISTData, BIT_WIDTH=8):\n",
                "        self.BIT_WIDTH = BIT_WIDTH\n",
                "        self.load_model()\n",
                "        # self.load_test_data()\n",
                "        self.x_train = data.x_train\n",
                "        self.y_train = data.y_train\n",
                "        self.x_test = data.x_test\n",
                "        self.y_test = data.y_test\n",
                "\n",
                "        self.float_predictions()\n",
                "        self.quantize_model()\n",
                "        self.quantized_predictions()\n",
                "\n",
                "    def load_model(self):\n",
                "        self.model = tf.keras.models.load_model(f'model/QAE_model8bits/KERAS_check_best_model.model')\n",
                "\n",
                "    # def load_test_data(self):\n",
                "        # (self.x_train, self.y_train), (self.x_test, self.y_test) = tf.keras.datasets.mnist.load_data()\n",
                "        # self.x_test = self.x_test.reshape(-1, 28, 28, 1).astype(np.float32) / 255\n",
                "\n",
                "    def float_predictions(self, n=6):\n",
                "        self.float_model_predictions = [self.model.predict(self.x_test[i:i+1]) for i in range(n)]\n",
                "\n",
                "    def representative_dataset(self):\n",
                "        for data in self.x_train:\n",
                "            # yield [np.array([data], dtype=np.float32)]\n",
                "            # print(f\"data: {data* (2 ** (self.BIT_WIDTH - 1))}\")\n",
                "            yield [np.array([data * (2 ** (self.BIT_WIDTH - 1))], dtype=np.float32)]\n",
                "\n",
                "    def quantize_model(self):\n",
                "        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n",
                "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "        converter.representative_dataset = self.representative_dataset\n",
                "        converter.target_spec.supported_types = [tf.int8]\n",
                "        converter.inference_input_type = tf.int8\n",
                "        converter.inference_output_type = tf.int8\n",
                "\n",
                "        tflite_quantized_model = converter.convert()\n",
                "        with open('quantized_model.tflite', 'wb') as f:\n",
                "            f.write(tflite_quantized_model)\n",
                "\n",
                "        self.interpreter = tf.lite.Interpreter(model_path='quantized_model.tflite')\n",
                "        self.input_details = self.interpreter.get_input_details()\n",
                "        self.output_details = self.interpreter.get_output_details()\n",
                "        self.interpreter.allocate_tensors()\n",
                "\n",
                "\n",
                "    def quantized_predictions(self, n=6):\n",
                "        quantized_model_predictions = []\n",
                "        input_shape = self.input_details[0]['shape']\n",
                "\n",
                "        for i in range(n):\n",
                "            # Prepare input data\n",
                "            input_data = np.array(self.x_test[i] * 255, dtype=np.int8).reshape(input_shape)\n",
                "            self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
                "\n",
                "            # Run inference\n",
                "            self.interpreter.invoke()\n",
                "\n",
                "            # Get output\n",
                "            output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
                "            output_data = output_data.astype(np.float32) / 255\n",
                "            quantized_model_predictions.append(output_data)\n",
                "\n",
                "        self.quantized_model_predictions = quantized_model_predictions\n",
                "\n",
                "model_obj = Model(data = data_zoom)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 102,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[-2.752086  ,  1.997776  ,  2.7898467 ,  9.44052   ,  3.7427683 ,\n",
                            "        -4.2579236 , -3.2069857 ,  1.1323516 ,  1.338767  ,  9.354033  ,\n",
                            "        19.511097  , 31.698778  , 30.946756  , 12.361761  ,  5.5769844 ,\n",
                            "         1.2727685 ,  7.662607  , 32.18009   , 54.106125  , 54.084885  ,\n",
                            "        52.09451   , 58.28578   , 19.370346  ,  5.0473223 ,  6.1245546 ,\n",
                            "        23.462564  , 32.79785   , 16.653275  , 34.669796  , 60.982483  ,\n",
                            "         6.219524  ,  0.9711714 ,  1.2746134 , 16.489004  , 26.107405  ,\n",
                            "        35.25992   , 73.35751   , 59.01702   , -0.49239063,  4.742521  ,\n",
                            "         0.4666829 ,  0.8794317 ,  3.2623491 , 14.964722  , 76.46785   ,\n",
                            "        24.488543  , -3.72138   ,  6.3352547 , -1.4701275 , -9.985825  ,\n",
                            "        -5.987548  , 24.469536  , 61.616566  , 14.400737  ,  4.6855793 ,\n",
                            "        -0.17710257, -1.7478358 , -0.3814392 ,  0.98126054, 26.37778   ,\n",
                            "        47.998955  , 16.075287  ,  7.4985876 ,  2.8173275 ]],\n",
                            "      dtype=float32)"
                        ]
                    },
                    "execution_count": 102,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_obj.float_model_predictions[0]*128"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 103,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[-30.619608 , -31.121569 , -26.603922 , -24.094118 , -10.541177 ,\n",
                            "        -10.039216 , -27.607843 , -29.113726 , -32.125492 , -34.133335 ,\n",
                            "        -28.109804 ,  -2.0078433,   9.537255 ,   0.       ,  -8.533334 ,\n",
                            "        -30.619608 , -31.121569 , -42.666668 , -19.576471 ,  -8.533334 ,\n",
                            "        -24.596079 , -35.137257 ,  -6.5254903, -20.580393 , -28.611765 ,\n",
                            "        -26.603922 ,   8.031373 , -11.043138 , -24.596079 , -36.14118  ,\n",
                            "          3.5137255, -24.596079 , -27.607843 ,  -7.027451 ,   1.5058824,\n",
                            "        -21.584314 , -30.117647 , -30.117647 ,  12.047059 , -23.592157 ,\n",
                            "        -25.09804  ,   3.0117648,  -1.0039216, -28.611765 , -34.635296 ,\n",
                            "         -3.5137255,   9.035295 , -32.627453 , -25.6      ,   1.5058824,\n",
                            "         12.047059 ,  -1.0039216,   6.5254903,   2.509804 , -24.094118 ,\n",
                            "        -30.619608 , -28.109804 , -21.082354 , -10.541177 , -10.039216 ,\n",
                            "        -22.588236 , -34.133335 , -32.125492 , -30.117647 ]],\n",
                            "      dtype=float32)"
                        ]
                    },
                    "execution_count": 103,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_obj.quantized_model_predictions[0]*128"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 101,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[ 0.21771501,  0.2587449 ,  0.22963881,  0.26198936,  0.11159332,\n",
                            "         0.04516635,  0.1906317 ,  0.23629747,  0.26143953,  0.33974507,\n",
                            "         0.3720383 ,  0.263333  ,  0.16726172,  0.09657626,  0.11023686,\n",
                            "         0.24915919,  0.30300137,  0.5847403 ,  0.57564527,  0.48920482,\n",
                            "         0.59914523,  0.72986746,  0.20231122,  0.20021653,  0.2713775 ,\n",
                            "         0.39114442,  0.19348812,  0.21637821,  0.46301466,  0.7587786 ,\n",
                            "         0.02113905,  0.19974414,  0.2256442 ,  0.1837223 ,  0.1921994 ,\n",
                            "         0.44409555,  0.8083997 ,  0.6963646 , -0.09796445,  0.22136468,\n",
                            "         0.19972439, -0.01665885,  0.03333024,  0.3404413 ,  0.86799335,\n",
                            "         0.21876772, -0.09966152,  0.30439615,  0.18851463, -0.08977896,\n",
                            "        -0.14089537,  0.19901139,  0.43039903,  0.09289791,  0.22484139,\n",
                            "         0.23783207,  0.20595288,  0.1617259 ,  0.09001904,  0.28450778,\n",
                            "         0.5514624 ,  0.39225486,  0.30956313,  0.2573045 ]],\n",
                            "      dtype=float32)"
                        ]
                    },
                    "execution_count": 101,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_obj.float_model_predictions[0] - model_obj.quantized_model_predictions[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# others"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 92,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Get the weights and biases of the loaded model\n",
                "# for layer in model.layers:\n",
                "#     weights = layer.get_weights()\n",
                "#     print(f'Layer {layer.name}:')\n",
                "#     # print(f'max {max(weights)}:')\n",
                "    \n",
                "#     if len(weights) == 2:\n",
                "#         w, b = weights\n",
                "#         print(f'  Weights:\\n{w}')\n",
                "#         # print(f'max {max(w)}:')\n",
                "#         print(f'  Biases:\\n{b}')\n",
                "#     else:\n",
                "#         print('  No weights and biases.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 93,
            "metadata": {},
            "outputs": [],
            "source": [
                "# def representative_dataset():\n",
                "#     for data in x_train:\n",
                "#         # yield [np.array([data], dtype=np.float32)]\n",
                "#         yield [np.array([data * (2 ** (BIT_WIDTH - 1))], dtype=np.float32)]\n",
                "\n",
                "\n",
                "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
                "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "# converter.representative_dataset = representative_dataset\n",
                "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
                "# converter.inference_input_type = tf.int8\n",
                "# converter.inference_output_type = tf.int8\n",
                "\n",
                "# quantized_tflite_model = converter.convert()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 94,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PARAMETERS_NAME = f\"{BIT_WIDTH}bit_quantized_{loss}loss\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 95,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Save the saved quantized weights\n",
                "# model.save_weights('quantized_weights.h5')\n",
                "\n",
                "# # Load the saved quantized weights\n",
                "# model.load_weights('quantized_weights.h5')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 96,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Save the quantized model\n",
                "# with open(f'quantized_model.tflite', 'wb') as f:\n",
                "#     f.write(quantized_tflite_model)\n",
                "\n",
                "# # Load the quantized model\n",
                "# interpreter = tf.lite.Interpreter(model_path='quantized_model.tflite')\n",
                "# interpreter.allocate_tensors()\n",
                "\n",
                "# # Get input and output details\n",
                "# input_details = interpreter.get_input_details()\n",
                "# output_details = interpreter.get_output_details()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 97,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import numpy as np\n",
                "# import tensorflow as tf\n",
                "# from tensorflow.keras.layers import Input, Dense\n",
                "# from tensorflow.keras.models import Model\n",
                "\n",
                "# model = autoencoder_generator(x_train)\n",
                "\n",
                "# # Load the saved quantized weights\n",
                "# model.load_weights('quantized_weights.h5')\n",
                "# # model.fit(x_train, y_train, epochs=10, batch_size=32)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.5"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "d1b75f63a51ab1e44c10e89cf3b718812d9c5e2447d39cb402b946ba7653bfcd"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}